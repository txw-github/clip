#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç”µè§†å‰§æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ - ä¸»ç¨‹åº
åŸºäºAIå¤§æ¨¡å‹çš„æ™ºèƒ½åˆ†æï¼Œé€‚åº”å„ç§å‰§æƒ…ç±»å‹
"""

import os
import sys
import json
import requests
import hashlib
import subprocess
from typing import List, Dict, Optional

class AIClipperSystem:
    def __init__(self):
        self.ai_config = self.load_ai_config()
        self.supported_models = [
            'claude-3-5-sonnet-20240620',
            'deepseek-r1', 
            'gemini-2.5-pro',
            'gpt-4o',
            'deepseek-chat'
        ]
        
        # ç›®å½•é…ç½®
        self.srt_folder = "srt"
        self.videos_folder = "videos"
        self.output_folder = "intelligent_clips"
        self.cache_folder = "cache"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.setup_directories()

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        dirs = [self.srt_folder, self.videos_folder, self.output_folder, 
               self.cache_folder, f"{self.cache_folder}/analysis", f"{self.cache_folder}/clips"]
        for directory in dirs:
            os.makedirs(directory, exist_ok=True)

    def load_ai_config(self) -> Dict:
        """åŠ è½½AIé…ç½®"""
        config_file = '.ai_config.json'
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass

        return {
            'enabled': False,
            'base_url': 'https://www.chataiapi.com/v1',
            'api_key': '',
            'model': 'claude-3-5-sonnet-20240620'
        }

    def save_ai_config(self, config: Dict):
        """ä¿å­˜AIé…ç½®"""
        with open('.ai_config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

    def setup_ai_config(self):
        """è®¾ç½®AIé…ç½®"""
        print("\nğŸ¤– æ™ºèƒ½AIåˆ†æé…ç½®")
        print("=" * 50)
        
        try:
            from api_config_helper import config_helper
            config = config_helper.interactive_setup()
            
            if config.get('enabled'):
                self.ai_config = config
                self.save_ai_config(config)
                print("âœ… AIé…ç½®æˆåŠŸï¼")
                return True
            else:
                print("âš ï¸ è·³è¿‡AIé…ç½®ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼")
                return False
                
        except ImportError:
            print("âŒ é…ç½®åŠ©æ‰‹æ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–é…ç½®")
            return self.setup_simple_ai_config()
        except Exception as e:
            print(f"âŒ é…ç½®è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def setup_simple_ai_config(self):
        """ç®€åŒ–çš„AIé…ç½®ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        print("\nğŸ“ ç®€åŒ–AIé…ç½®")
        print("=" * 30)
        
        api_key = input("è¾“å…¥APIå¯†é’¥: ").strip()
        if not api_key:
            print("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
            return False

        print("\né€‰æ‹©APIç±»å‹:")
        print("1. ä¸­è½¬API (æ¨èï¼Œå¦‚ChatAI)")
        print("2. å®˜æ–¹API (éœ€è¦é­”æ³•ä¸Šç½‘)")
        
        api_type = input("è¯·é€‰æ‹© (1-2): ").strip()
        
        if api_type == "1":
            base_url = input("APIåœ°å€ (å›è½¦ä½¿ç”¨ https://www.chataiapi.com/v1): ").strip()
            if not base_url:
                base_url = "https://www.chataiapi.com/v1"
            model = input("æ¨¡å‹åç§° (å›è½¦ä½¿ç”¨ deepseek-r1): ").strip()
            if not model:
                model = "deepseek-r1"
                
            config = {
                'enabled': True,
                'provider': 'chataiapi',
                'base_url': base_url,
                'api_key': api_key,
                'model': model,
                'api_type': 'openai_compatible'
            }
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return False

        # æµ‹è¯•è¿æ¥
        print("ğŸ” æµ‹è¯•APIè¿æ¥...")
        if self.test_ai_connection(config):
            self.ai_config = config
            self.save_ai_config(config)
            print("âœ… AIé…ç½®æˆåŠŸï¼")
            return True
        else:
            print("âŒ APIè¿æ¥å¤±è´¥")
            return False

    def test_ai_connection(self, config: Dict) -> bool:
        """æµ‹è¯•AIè¿æ¥"""
        try:
            payload = {
                "model": config['model'],
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰§æƒ…åˆ†æå¸ˆ"},
                    {"role": "user", "content": "æµ‹è¯•è¿æ¥"}
                ],
                "max_tokens": 10
            }

            url = config['base_url'] + "/chat/completions"
            headers = {
                'Accept': 'application/json',
                'Authorization': f'Bearer {config["api_key"]}',
                'User-Agent': 'TV-Clipper/1.0.0',
                'Content-Type': 'application/json'
            }

            response = requests.post(url, headers=headers, json=payload, timeout=10)
            return response.status_code == 200
        except:
            return False

    def get_file_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºç¼“å­˜"""
        return hashlib.md5(content.encode()).hexdigest()

    def load_analysis_cache(self, cache_key: str) -> Optional[Dict]:
        """åŠ è½½åˆ†æç¼“å­˜"""
        cache_path = os.path.join(self.cache_folder, "analysis", f"{cache_key}.json")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return None

    def save_analysis_cache(self, cache_key: str, analysis: Dict):
        """ä¿å­˜åˆ†æç¼“å­˜"""
        cache_path = os.path.join(self.cache_folder, "analysis", f"{cache_key}.json")
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def call_ai_api(self, prompt: str, system_prompt: str = "ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰§æƒ…åˆ†æå¸ˆ") -> Optional[str]:
        """è°ƒç”¨AI API"""
        if not self.ai_config.get('enabled'):
            return None

        try:
            payload = {
                "model": self.ai_config['model'],
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 3000,
                "temperature": 0.7
            }

            url = self.ai_config['base_url'] + "/chat/completions"
            headers = {
                'Accept': 'application/json',
                'Authorization': f'Bearer {self.ai_config["api_key"]}',
                'User-Agent': 'TV-Clipper/1.0.0',
                'Content-Type': 'application/json'
            }

            response = requests.post(url, headers=headers, json=payload, timeout=60)

            if response.status_code == 200:
                data = response.json()
                message = data['choices'][0]['message']
                
                # å¤„ç†deepseek-r1çš„ç‰¹æ®Šæ ¼å¼
                if 'reasoning_content' in message:
                    print(f"ğŸ§  AIæ€è€ƒè¿‡ç¨‹: {message['reasoning_content'][:100]}...")
                    
                return message.get('content', '')
            else:
                print(f"âš  APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return None

        except Exception as e:
            print(f"âš  AIè°ƒç”¨å¼‚å¸¸: {e}")
            return None

    def parse_srt_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶"""
        try:
            # å¤šç¼–ç å°è¯•
            for encoding in ['utf-8', 'gbk', 'gb2312']:
                try:
                    with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                        content = f.read()
                    break
                except:
                    continue

            # æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£
            corrections = {
                'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
                'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
                'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©',
                'è½è­‰æœƒ': 'å¬è¯ä¼š', 'è¾¯è­·': 'è¾©æŠ¤', 'å¯©åˆ¤': 'å®¡åˆ¤', 'èª¿æŸ¥': 'è°ƒæŸ¥'
            }

            for old, new in corrections.items():
                content = content.replace(old, new)

            # è§£æå­—å¹•å—
            import re
            blocks = re.split(r'\n\s*\n', content.strip())
            subtitles = []

            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    try:
                        index = int(lines[0])
                        time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                        if time_match:
                            start_time = time_match.group(1)
                            end_time = time_match.group(2)
                            text = '\n'.join(lines[2:])

                            subtitles.append({
                                'index': index,
                                'start': start_time,
                                'end': end_time,
                                'text': text,
                                'start_seconds': self.time_to_seconds(start_time),
                                'end_seconds': self.time_to_seconds(end_time),
                                'episode': os.path.basename(filepath)
                            })
                    except (ValueError, IndexError):
                        continue

            return subtitles
        except Exception as e:
            print(f"âŒ è§£æå­—å¹•æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return []

    def merge_subtitles_intelligently(self, subtitles: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½åˆå¹¶å­—å¹•ï¼Œæä¾›å®Œæ•´ä¸Šä¸‹æ–‡ç»™AIå†³ç­–"""
        if not subtitles:
            return []

        merged_segments = []
        current_segment = {'texts': [], 'start_time': '', 'end_time': '', 'subtitles': []}
        
        for i, subtitle in enumerate(subtitles):
            current_segment['texts'].append(subtitle['text'])
            current_segment['subtitles'].append(subtitle)
            
            if not current_segment['start_time']:
                current_segment['start_time'] = subtitle['start']
            current_segment['end_time'] = subtitle['end']
            
            # åŠ¨æ€å†³å®šåˆ†æ®µ - é‡åˆ°è¾ƒé•¿åœé¡¿æˆ–è¶³å¤Ÿå†…å®¹æ—¶åˆ†æ®µ
            next_subtitle = subtitles[i + 1] if i + 1 < len(subtitles) else None
            should_split = False
            
            if next_subtitle:
                time_gap = next_subtitle['start_seconds'] - subtitle['end_seconds']
                segment_length = len(' '.join(current_segment['texts']))
                
                # åˆ†æ®µæ¡ä»¶ï¼šæ—¶é—´é—´éš”å¤§äº3ç§’ æˆ– æ–‡æœ¬é•¿åº¦è¶…è¿‡800å­— æˆ– é‡åˆ°åœºæ™¯è½¬æ¢æ ‡è¯†
                if (time_gap > 3 or segment_length > 800 or 
                    any(keyword in subtitle['text'] for keyword in ['åœºæ™¯', 'åˆ‡æ¢', 'å¦ä¸€è¾¹', 'åŒæ—¶', 'å›åˆ°'])):
                    should_split = True
            else:
                should_split = True  # æœ€åä¸€ä¸ªå­—å¹•
            
            if should_split:
                merged_segments.append({
                    'segment_text': ' '.join(current_segment['texts']),
                    'start_time': current_segment['start_time'],
                    'end_time': current_segment['end_time'],
                    'duration': self.time_to_seconds(current_segment['end_time']) - 
                              self.time_to_seconds(current_segment['start_time']),
                    'subtitle_count': len(current_segment['subtitles']),
                    'position_ratio': i / len(subtitles)
                })
                current_segment = {'texts': [], 'start_time': '', 'end_time': '', 'subtitles': []}

        return merged_segments

    def ai_analyze_complete_episode(self, segments: List[Dict], episode_file: str) -> Dict:
        """AIåˆ†æå®Œæ•´å‰§é›†ï¼Œæ”¯æŒç¼“å­˜"""
        # ç”Ÿæˆç¼“å­˜key
        content_for_hash = json.dumps([s['segment_text'] for s in segments], sort_keys=True)
        cache_key = self.get_file_hash(content_for_hash + episode_file)
        
        # å°è¯•åŠ è½½ç¼“å­˜
        cached_analysis = self.load_analysis_cache(cache_key)
        if cached_analysis:
            print(f"  ğŸ“‹ ä½¿ç”¨ç¼“å­˜åˆ†æç»“æœ")
            return cached_analysis

        if not self.ai_config.get('enabled'):
            return self.fallback_analysis(segments, episode_file)

        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        episode_context = self.build_complete_context(segments, episode_file)
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å½±è§†å‰ªè¾‘å¸ˆå’Œå‰§æƒ…åˆ†æä¸“å®¶ã€‚è¯·æ·±åº¦åˆ†æè¿™ä¸€é›†ç”µè§†å‰§çš„å®Œæ•´å†…å®¹ï¼Œæ™ºèƒ½è¯†åˆ«å‡ºé€‚åˆåˆ¶ä½œçŸ­è§†é¢‘çš„ç²¾å½©ç‰‡æ®µã€‚

ã€å‰§é›†ä¿¡æ¯ã€‘: {episode_file}
ã€å®Œæ•´å‰§æƒ…ä¸Šä¸‹æ–‡ã€‘:
{episode_context}

è¯·è¿›è¡Œä¸“ä¸šåˆ†æï¼š

1. **æ·±åº¦å‰§æƒ…ç†è§£**:
   - åˆ†æä¸»è¦å‰§æƒ…çº¿å’Œè§’è‰²å…³ç³»
   - è¯†åˆ«å…³é”®å†²çªç‚¹å’Œæƒ…æ„Ÿè½¬æŠ˜
   - ç†è§£å‰§æƒ…çš„å› æœå…³ç³»å’Œå‘å±•è„‰ç»œ

2. **æ™ºèƒ½ç‰‡æ®µè¯†åˆ«** (3-5ä¸ªç²¾å½©ç‰‡æ®µ):
   - æ¯ä¸ªç‰‡æ®µ2-3åˆ†é’Ÿï¼Œæœ‰å®Œæ•´æˆå‰§ç»“æ„
   - ä¼˜å…ˆé€‰æ‹©æˆå‰§å†²çªã€æƒ…æ„Ÿé«˜æ½®ã€çœŸç›¸æ­éœ²ç­‰ç²¾å½©å†…å®¹
   - ç¡®ä¿ç‰‡æ®µé—´é€»è¾‘è¿è´¯ï¼Œèƒ½å®Œæ•´å™è¿°æœ¬é›†æ•…äº‹
   - è€ƒè™‘çŸ­è§†é¢‘è§‚ä¼—çš„è§‚çœ‹ä¹ æƒ¯å’Œå¸å¼•åŠ›

3. **ä¸“ä¸šæ—ç™½è®¾è®¡**:
   - ä¸ºæ¯ä¸ªç‰‡æ®µè®¾è®¡å¼•äººå…¥èƒœçš„è§£è¯´
   - è§£é‡ŠèƒŒæ™¯ã€äººç‰©åŠ¨æœºã€å‰§æƒ…æ„ä¹‰
   - è¯­è¨€è¦ç”ŸåŠ¨æœ‰è¶£ï¼Œé€‚åˆç°ä»£è§‚ä¼—

4. **è¿è´¯æ€§ä¿è¯**:
   - å¤„ç†å¯èƒ½çš„å‰§æƒ…åè½¬å’Œä¼ç¬”
   - ç¡®ä¿ä¸å‰åé›†çš„é€»è¾‘å…³è”
   - ä¿è¯ä¸€å¥è¯è®²å®Œçš„å®Œæ•´æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "episode_analysis": {{
        "main_storyline": "ä¸»è¦å‰§æƒ…çº¿",
        "key_characters": ["ä¸»è¦è§’è‰²1", "ä¸»è¦è§’è‰²2"],
        "dramatic_elements": ["æˆå‰§å…ƒç´ 1", "æˆå‰§å…ƒç´ 2"],
        "emotional_tone": "æƒ…æ„ŸåŸºè°ƒ"
    }},
    "intelligent_clips": [
        {{
            "clip_id": 1,
            "title": "ç‰‡æ®µæ ‡é¢˜",
            "start_time": "00:05:30,000",
            "end_time": "00:08:15,000",
            "duration_seconds": 165,
            "significance": "å‰§æƒ…æ„ä¹‰å’Œä»·å€¼",
            "hook_factor": "å¸å¼•è§‚ä¼—çš„æ ¸å¿ƒå–ç‚¹",
            "narrative_structure": {{
                "setup": "èƒŒæ™¯é“ºå«",
                "conflict": "æ ¸å¿ƒå†²çª", 
                "climax": "é«˜æ½®éƒ¨åˆ†",
                "resolution": "ç»“æœè§£å†³"
            }},
            "professional_commentary": {{
                "opening_hook": "å¼€åœºå¸å¼•è§£è¯´",
                "background_context": "èƒŒæ™¯è§£é‡Š",
                "plot_analysis": "å‰§æƒ…åˆ†æ",
                "emotional_impact": "æƒ…æ„Ÿå†²å‡»è¯´æ˜",
                "conclusion": "æ€»ç»“å‡å"
            }},
            "connection_next": "ä¸ä¸‹ä¸ªç‰‡æ®µçš„é€»è¾‘è¿æ¥"
        }}
    ],
    "episode_summary": "æœ¬é›†å®Œæ•´å‰§æƒ…æ¦‚è¿°",
    "continuity_notes": "ä¸å‰åé›†çš„è¿è´¯æ€§è¯´æ˜",
    "series_context": "åœ¨æ•´ä¸ªå‰§é›†ä¸­çš„ä½ç½®å’Œä½œç”¨"
}}"""

        print(f"  ğŸ¤– AIæ·±åº¦åˆ†æä¸­...")
        response = self.call_ai_api(prompt)
        
        if response:
            try:
                # è§£æJSONå“åº”
                if "```json" in response:
                    json_start = response.find("```json") + 7
                    json_end = response.find("```", json_start)
                    json_text = response[json_start:json_end]
                else:
                    start = response.find("{")
                    end = response.rfind("}") + 1
                    json_text = response[start:end]
                
                analysis = json.loads(json_text)
                processed_analysis = self.process_analysis_result(analysis, segments, episode_file)
                
                # ä¿å­˜åˆ°ç¼“å­˜
                self.save_analysis_cache(cache_key, processed_analysis)
                print(f"  âœ… AIåˆ†æå®Œæˆï¼Œå·²ç¼“å­˜")
                
                return processed_analysis
                
            except Exception as e:
                print(f"âš  AIåˆ†æç»“æœè§£æå¤±è´¥: {e}")
                return self.fallback_analysis(segments, episode_file)
        
        return self.fallback_analysis(segments, episode_file)

    def build_complete_context(self, segments: List[Dict], episode_file: str) -> str:
        """æ„å»ºå®Œæ•´çš„å‰§é›†ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, segment in enumerate(segments[:30]):  # é™åˆ¶é•¿åº¦é¿å…APIè¶…é™
            time_info = f"[æ—¶é—´æ®µ {i+1}: {segment['start_time']} - {segment['end_time']}]"
            position_info = f"[å‰§é›†ä½ç½®: {segment['position_ratio']*100:.0f}%]"
            
            # æˆªå–é€‚å½“é•¿åº¦çš„æ–‡æœ¬
            text = segment['segment_text']
            if len(text) > 400:
                text = text[:400] + "..."
            
            context_parts.append(f"{time_info} {position_info}\n{text}")
        
        return "\n\n".join(context_parts)

    def process_analysis_result(self, analysis: Dict, segments: List[Dict], episode_file: str) -> Dict:
        """å¤„ç†AIåˆ†æç»“æœ"""
        processed_clips = []
        
        for clip in analysis.get('intelligent_clips', []):
            # éªŒè¯å’Œä¼˜åŒ–æ—¶é—´ç 
            optimized_clip = self.optimize_clip_timing(clip, segments)
            if optimized_clip:
                processed_clips.append(optimized_clip)
        
        # æå–é›†æ•°
        import re
        episode_match = re.search(r'[Ee](\d+)', episode_file)
        episode_number = episode_match.group(1) if episode_match else "00"
        
        return {
            'episode_file': episode_file,
            'episode_number': episode_number,
            'episode_analysis': analysis.get('episode_analysis', {}),
            'clips': processed_clips,
            'episode_summary': analysis.get('episode_summary', ''),
            'continuity_notes': analysis.get('continuity_notes', ''),
            'series_context': analysis.get('series_context', ''),
            'ai_generated': True,
            'total_clips': len(processed_clips)
        }

    def optimize_clip_timing(self, clip: Dict, segments: List[Dict]) -> Optional[Dict]:
        """ä¼˜åŒ–å‰ªè¾‘æ—¶é—´ï¼Œç¡®ä¿å®Œæ•´æ€§"""
        start_time = clip.get('start_time', '')
        end_time = clip.get('end_time', '')
        
        if not start_time or not end_time:
            return None
        
        start_seconds = self.time_to_seconds(start_time)
        end_seconds = self.time_to_seconds(end_time)
        
        # æ‰¾åˆ°æœ€ä½³çš„segmentè¾¹ç•Œ
        best_start = start_seconds
        best_end = end_seconds
        
        # å‘å‰æ‰©å±•ç¡®ä¿å®Œæ•´å¥å­
        for segment in segments:
            seg_start = self.time_to_seconds(segment['start_time'])
            seg_end = self.time_to_seconds(segment['end_time'])
            
            if seg_start <= start_seconds <= seg_end:
                best_start = seg_start
            if seg_start <= end_seconds <= seg_end:
                best_end = seg_end
        
        # æ·»åŠ ç¼“å†²ç¡®ä¿å®Œæ•´æ€§
        buffer = 2.0
        final_start = max(0, best_start - buffer)
        final_end = best_end + buffer
        
        return {
            'clip_id': clip.get('clip_id', 1),
            'title': clip.get('title', 'ç²¾å½©ç‰‡æ®µ'),
            'start_time': self.seconds_to_time(final_start),
            'end_time': self.seconds_to_time(final_end),
            'duration': final_end - final_start,
            'significance': clip.get('significance', ''),
            'hook_factor': clip.get('hook_factor', ''),
            'narrative_structure': clip.get('narrative_structure', {}),
            'professional_commentary': clip.get('professional_commentary', {}),
            'connection_next': clip.get('connection_next', '')
        }

    def fallback_analysis(self, segments: List[Dict], episode_file: str) -> Dict:
        """å¤‡ç”¨åˆ†æï¼ˆæ— AIæ—¶ï¼‰"""
        high_intensity_segments = []
        
        keywords = ['çªç„¶', 'å‘ç°', 'çœŸç›¸', 'ç§˜å¯†', 'ä¸å¯èƒ½', 'ä¸ºä»€ä¹ˆ', 'æ€äºº', 'æ­»äº†', 
                   'æ•‘å‘½', 'å±é™©', 'å®Œäº†', 'éœ‡æƒŠ', 'æ„¤æ€’', 'å“­', 'å´©æºƒ', 'çˆ±ä½ ', 'åˆ†æ‰‹',
                   'å››äºŒå…«æ¡ˆ', '628æ—§æ¡ˆ', 'å¬è¯ä¼š', 'è¾©æŠ¤', 'æ£€å¯Ÿå®˜', 'æ³•å®˜']
        
        for i, segment in enumerate(segments):
            score = 0
            text = segment['segment_text']
            
            for keyword in keywords:
                score += text.count(keyword) * 3
            
            score += text.count('ï¼') * 2 + text.count('ï¼Ÿ') * 2 + text.count('...') * 1
            
            if score >= 8 and segment['duration'] >= 60:  # è‡³å°‘1åˆ†é’Ÿ
                high_intensity_segments.append({
                    'segment': segment,
                    'score': score,
                    'index': i
                })
        
        high_intensity_segments.sort(key=lambda x: x['score'], reverse=True)
        selected_segments = high_intensity_segments[:4]  # é€‰æ‹©å‰4ä¸ª
        
        clips = []
        for i, item in enumerate(selected_segments):
            segment = item['segment']
            clips.append({
                'clip_id': i + 1,
                'title': f"ç²¾å½©ç‰‡æ®µ{i + 1}",
                'start_time': segment['start_time'],
                'end_time': segment['end_time'],
                'duration': segment['duration'],
                'significance': 'åŸºäºå…³é”®è¯è¯†åˆ«çš„æˆå‰§æ€§ç‰‡æ®µ',
                'hook_factor': 'åŒ…å«å¼ºçƒˆæˆå‰§å†²çª',
                'narrative_structure': {
                    'setup': 'å‰§æƒ…èƒŒæ™¯',
                    'conflict': 'æ ¸å¿ƒå†²çª',
                    'climax': 'æƒ…æ„Ÿé«˜æ½®',
                    'resolution': 'æš‚æ—¶è§£å†³'
                },
                'professional_commentary': {
                    'opening_hook': f"åœ¨è¿™ä¸ªå…³é”®ç‰‡æ®µä¸­",
                    'background_context': "å‰§æƒ…å‘å±•åˆ°é‡è¦èŠ‚ç‚¹",
                    'plot_analysis': "è§’è‰²é¢ä¸´é‡è¦é€‰æ‹©",
                    'emotional_impact': "æƒ…æ„Ÿè¾¾åˆ°é«˜æ½®",
                    'conclusion': "ä¸ºåç»­å‘å±•åŸ‹ä¸‹ä¼ç¬”"
                },
                'connection_next': 'ä¸ä¸‹ä¸ªç‰‡æ®µå½¢æˆé€»è¾‘é€’è¿›'
            })
        
        import re
        episode_match = re.search(r'[Ee](\d+)', episode_file)
        episode_number = episode_match.group(1) if episode_match else "00"
        
        return {
            'episode_file': episode_file,
            'episode_number': episode_number,
            'episode_analysis': {
                'main_storyline': 'å‰§æƒ…å‘å±•ä¸å†²çª',
                'key_characters': ['ä¸»è¦è§’è‰²'],
                'dramatic_elements': ['æˆå‰§å†²çª', 'æƒ…æ„Ÿè½¬æŠ˜'],
                'emotional_tone': 'ç´§å¼ æ¿€çƒˆ'
            },
            'clips': clips,
            'episode_summary': 'æœ¬é›†åŒ…å«å¤šä¸ªå…³é”®å‰§æƒ…è½¬æŠ˜å’Œæƒ…æ„Ÿé«˜æ½®',
            'continuity_notes': 'ä¸å‰åé›†ä¿æŒå‰§æƒ…è¿è´¯æ€§',
            'series_context': 'æ¨è¿›ä¸»çº¿å‰§æƒ…å‘å±•',
            'ai_generated': False,
            'total_clips': len(clips)
        }

    def find_matching_video(self, episode_file: str) -> Optional[str]:
        """æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘æ–‡ä»¶"""
        base_name = os.path.splitext(episode_file)[0]
        video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv']
        
        # ç²¾ç¡®åŒ¹é…
        for ext in video_extensions:
            video_path = os.path.join(self.videos_folder, base_name + ext)
            if os.path.exists(video_path):
                return video_path
        
        # é›†æ•°åŒ¹é…
        import re
        episode_match = re.search(r'[Ee](\d+)', base_name)
        if episode_match:
            episode_num = episode_match.group(1)
            for filename in os.listdir(self.videos_folder):
                if any(filename.lower().endswith(ext) for ext in video_extensions):
                    video_match = re.search(r'[Ee](\d+)', filename)
                    if video_match and video_match.group(1) == episode_num:
                        return os.path.join(self.videos_folder, filename)
        
        return None

    def create_video_clips(self, video_file: str, analysis: Dict) -> List[str]:
        """åˆ›å»ºè§†é¢‘ç‰‡æ®µï¼Œæ”¯æŒç¼“å­˜å’Œæ–­ç‚¹ç»­ä¼ """
        created_clips = []
        episode_number = analysis['episode_number']
        
        print(f"  ğŸ¬ å¼€å§‹å‰ªè¾‘ {len(analysis['clips'])} ä¸ªç‰‡æ®µ")
        
        for clip in analysis['clips']:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            clip_filename = self.generate_clip_filename(episode_number, clip)
            clip_path = os.path.join(self.output_folder, clip_filename)
            
            if os.path.exists(clip_path) and os.path.getsize(clip_path) > 0:
                print(f"    âœ… ç‰‡æ®µå·²å­˜åœ¨: {clip_filename}")
                created_clips.append(clip_path)
                continue
            
            # æ‰§è¡Œå‰ªè¾‘
            if self.create_single_clip(video_file, clip, clip_path):
                created_clips.append(clip_path)
                # åˆ›å»ºæ—ç™½æ–‡ä»¶
                self.create_commentary_file(clip_path, clip)
        
        return created_clips

    def generate_clip_filename(self, episode_number: str, clip: Dict) -> str:
        """ç”Ÿæˆå‰ªè¾‘æ–‡ä»¶å"""
        safe_title = self.sanitize_filename(clip['title'])
        return f"E{episode_number}_C{clip['clip_id']:02d}_{safe_title}.mp4"

    def sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
        import re
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬ç¬¦å·
        safe_name = re.sub(r'[^\w\u4e00-\u9fff\-_]', '_', filename)
        return safe_name[:30]  # é™åˆ¶é•¿åº¦

    def create_single_clip(self, video_file: str, clip: Dict, output_path: str, max_retries: int = 3) -> bool:
        """åˆ›å»ºå•ä¸ªè§†é¢‘ç‰‡æ®µï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(max_retries):
            try:
                start_seconds = self.time_to_seconds(clip['start_time'])
                duration = clip['duration']
                
                print(f"    ğŸ¬ å‰ªè¾‘ç‰‡æ®µ {clip['clip_id']}: {clip['title']} (å°è¯• {attempt + 1})")
                print(f"        æ—¶é—´: {clip['start_time']} --> {clip['end_time']} ({duration:.1f}ç§’)")
                
                cmd = [
                    'ffmpeg',
                    '-i', video_file,
                    '-ss', str(start_seconds),
                    '-t', str(duration),
                    '-c:v', 'libx264',
                    '-c:a', 'aac',
                    '-preset', 'fast',
                    '-crf', '23',
                    '-avoid_negative_ts', 'make_zero',
                    output_path,
                    '-y'
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
                
                if result.returncode == 0 and os.path.exists(output_path):
                    file_size = os.path.getsize(output_path) / (1024*1024)
                    if file_size > 0.1:  # è‡³å°‘100KB
                        print(f"        âœ… å‰ªè¾‘æˆåŠŸ: {file_size:.1f}MB")
                        return True
                    else:
                        print(f"        âŒ æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½å¤±è´¥")
                        if os.path.exists(output_path):
                            os.remove(output_path)
                else:
                    error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                    print(f"        âŒ å‰ªè¾‘å¤±è´¥ (å°è¯• {attempt + 1}): {error_msg}")
                    
            except subprocess.TimeoutExpired:
                print(f"        âŒ å‰ªè¾‘è¶…æ—¶ (å°è¯• {attempt + 1})")
            except Exception as e:
                print(f"        âŒ å‰ªè¾‘å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
            
            if attempt < max_retries - 1:
                print(f"        â° ç­‰å¾…3ç§’åé‡è¯•...")
                import time
                time.sleep(3)
        
        print(f"    âŒ å‰ªè¾‘æœ€ç»ˆå¤±è´¥: {clip['title']}")
        return False

    def create_commentary_file(self, video_path: str, clip: Dict):
        """åˆ›å»ºä¸“ä¸šæ—ç™½æ–‡ä»¶"""
        commentary_path = video_path.replace('.mp4', '_æ—ç™½è§£è¯´.txt')
        
        commentary = clip.get('professional_commentary', {})
        
        content = f"""ğŸ“º ä¸“ä¸šæ—ç™½è§£è¯´ - {clip['title']}
{'='*60}

ğŸ¯ ç‰‡æ®µä»·å€¼: {clip.get('significance', 'æœªçŸ¥')}
ğŸª å¸å¼•å–ç‚¹: {clip.get('hook_factor', 'æœªçŸ¥')}

ğŸ“– å™äº‹ç»“æ„:
â”œâ”€ èƒŒæ™¯é“ºå«: {clip.get('narrative_structure', {}).get('setup', 'æš‚æ— ')}
â”œâ”€ æ ¸å¿ƒå†²çª: {clip.get('narrative_structure', {}).get('conflict', 'æš‚æ— ')}
â”œâ”€ é«˜æ½®éƒ¨åˆ†: {clip.get('narrative_structure', {}).get('climax', 'æš‚æ— ')}
â””â”€ ç»“æœè§£å†³: {clip.get('narrative_structure', {}).get('resolution', 'æš‚æ— ')}

ğŸ™ï¸ ä¸“ä¸šè§£è¯´è¯:

ã€å¼€åœºå¼•å…¥ã€‘
{commentary.get('opening_hook', 'åœ¨è¿™ä¸ªç²¾å½©ç‰‡æ®µä¸­...')}

ã€èƒŒæ™¯è§£é‡Šã€‘
{commentary.get('background_context', 'æ•…äº‹èƒŒæ™¯è¯´æ˜...')}

ã€å‰§æƒ…åˆ†æã€‘
{commentary.get('plot_analysis', 'å‰§æƒ…æ·±åº¦åˆ†æ...')}

ã€æƒ…æ„Ÿå†²å‡»ã€‘
{commentary.get('emotional_impact', 'æƒ…æ„Ÿå†²å‡»è¯´æ˜...')}

ã€æ€»ç»“å‡åã€‘
{commentary.get('conclusion', 'å†…å®¹æ€»ç»“å’Œå‡å...')}

ğŸ”— ä¸ä¸‹ç‰‡æ®µè¿æ¥: {clip.get('connection_next', 'æš‚æ— ')}

â±ï¸ æ—¶é—´è½´: {clip.get('start_time', '')} --> {clip.get('end_time', '')} ({clip.get('duration', 0):.1f}ç§’)
"""
        
        try:
            with open(commentary_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"        ğŸ“ ç”Ÿæˆæ—ç™½: {os.path.basename(commentary_path)}")
        except Exception as e:
            print(f"        âš ï¸ ç”Ÿæˆæ—ç™½å¤±è´¥: {e}")

    def run_advanced_clipping(self):
        """è¿è¡Œé«˜çº§æ™ºèƒ½å‰ªè¾‘ï¼ˆæ•´åˆæ‰€æœ‰æ”¹è¿›ï¼‰"""
        print("ğŸš€ å¯åŠ¨é«˜çº§æ™ºèƒ½å‰ªè¾‘ç³»ç»Ÿ")
        print("=" * 60)
        print("âœ¨ é›†æˆæ‰€æœ‰æ”¹è¿›åŠŸèƒ½:")
        print("  â€¢ AIæ·±åº¦å‰§æƒ…ç†è§£ï¼Œéå›ºå®šè§„åˆ™")
        print("  â€¢ å®Œæ•´ä¸Šä¸‹æ–‡åˆ†æï¼Œé¿å…å‰§æƒ…å‰²è£‚")
        print("  â€¢ æ™ºèƒ½å¤šæ®µè¯†åˆ«ï¼Œæ¯é›†3-5ä¸ªçŸ­è§†é¢‘")
        print("  â€¢ è‡ªåŠ¨è§†é¢‘å‰ªè¾‘+ä¸“ä¸šæ—ç™½è§£è¯´")
        print("  â€¢ ç¼“å­˜ç³»ç»Ÿï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
        print("  â€¢ ä¿è¯å‰§æƒ…è¿è´¯æ€§å’Œå®Œæ•´æ€§")
        print("  â€¢ ä¸€å¥è¯è®²å®Œçš„å®Œæ•´ä¿è¯")
        
        # æ£€æŸ¥æ–‡ä»¶
        srt_files = self.get_srt_files()
        if not srt_files:
            print("\nâŒ srtç›®å½•ä¸­æ²¡æœ‰å­—å¹•æ–‡ä»¶")
            print("è¯·å°†.srtå­—å¹•æ–‡ä»¶æ”¾å…¥srt/ç›®å½•")
            return
        
        video_files = self.get_video_files()
        if not video_files:
            print("âŒ videosç›®å½•ä¸­æ²¡æœ‰è§†é¢‘æ–‡ä»¶")
            print("è¯·å°†è§†é¢‘æ–‡ä»¶æ”¾å…¥videos/ç›®å½•")
            return
        
        print(f"\nâœ… æ‰¾åˆ° {len(srt_files)} ä¸ªå­—å¹•æ–‡ä»¶")
        print(f"âœ… æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        ai_status = "ğŸ¤– AIæ™ºèƒ½åˆ†æ" if self.ai_config.get('enabled') else "ğŸ“ è§„åˆ™åˆ†æ"
        print(f"ğŸ¯ åˆ†ææ¨¡å¼: {ai_status}")
        
        if self.ai_config.get('enabled'):
            print(f"ğŸ¤– AIæ¨¡å‹: {self.ai_config.get('model', 'unknown')}")
        
        # å¤„ç†æ‰€æœ‰å‰§é›†
        all_results = []
        total_clips = 0
        
        for srt_file in srt_files:
            print(f"\n{'='*60}")
            print(f"ğŸ“º å¤„ç†å‰§é›†: {srt_file}")
            print(f"{'='*60}")
            
            try:
                # 1. è§£æå­—å¹•
                srt_path = os.path.join(self.srt_folder, srt_file)
                subtitles = self.parse_srt_file(srt_path)
                
                if not subtitles:
                    print("  âŒ å­—å¹•è§£æå¤±è´¥")
                    continue
                
                print(f"  ğŸ“ è§£æå­—å¹•: {len(subtitles)} æ¡")
                
                # 2. æ™ºèƒ½åˆå¹¶å­—å¹•æ®µè½
                segments = self.merge_subtitles_intelligently(subtitles)
                print(f"  ğŸ“‘ æ™ºèƒ½åˆå¹¶: {len(segments)} ä¸ªè¿è´¯æ®µè½")
                
                # 3. AIæ·±åº¦åˆ†æ
                analysis = self.ai_analyze_complete_episode(segments, srt_file)
                
                if not analysis['clips']:
                    print("  âŒ æœªè¯†åˆ«åˆ°ç²¾å½©ç‰‡æ®µ")
                    continue
                
                print(f"  ğŸ¯ è¯†åˆ«ç²¾å½©ç‰‡æ®µ: {len(analysis['clips'])} ä¸ª")
                print(f"  ğŸ“– å‰§æƒ…ä¸»çº¿: {analysis['episode_analysis'].get('main_storyline', 'æœªçŸ¥')}")
                
                # 4. æŸ¥æ‰¾åŒ¹é…è§†é¢‘
                video_file = self.find_matching_video(srt_file)
                if not video_file:
                    print("  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶")
                    continue
                
                print(f"  ğŸ“¹ åŒ¹é…è§†é¢‘: {os.path.basename(video_file)}")
                
                # 5. åˆ›å»ºè§†é¢‘ç‰‡æ®µ
                created_clips = self.create_video_clips(video_file, analysis)
                
                if created_clips:
                    print(f"  âœ… æˆåŠŸåˆ›å»º: {len(created_clips)} ä¸ªçŸ­è§†é¢‘")
                    total_clips += len(created_clips)
                    
                    # 6. ç”Ÿæˆå‰§é›†è¯´æ˜
                    self.create_episode_report(analysis, created_clips)
                    
                    all_results.append({
                        'episode': srt_file,
                        'analysis': analysis,
                        'clips': created_clips
                    })
                else:
                    print("  âŒ è§†é¢‘å‰ªè¾‘å¤±è´¥")
                    
            except Exception as e:
                print(f"  âŒ å¤„ç†å‡ºé”™: {e}")
        
        # 7. ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        if all_results:
            self.create_complete_report(all_results)
            
            print(f"\nğŸ‰ é«˜çº§æ™ºèƒ½å‰ªè¾‘å®Œæˆï¼")
            print(f"{'='*60}")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  âœ… æˆåŠŸå¤„ç†: {len(all_results)} é›†")
            print(f"  ğŸ¬ åˆ›å»ºçŸ­è§†é¢‘: {total_clips} ä¸ª")
            print(f"  ğŸ“ è¾“å‡ºç›®å½•: {self.output_folder}/")
            print(f"  ğŸ“ åŒ…å«ä¸“ä¸šæ—ç™½è§£è¯´æ–‡ä»¶")
            print(f"  ğŸ’¾ åˆ†æç»“æœå·²ç¼“å­˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ")
            print(f"  ğŸ”„ ä¿è¯è·¨é›†å‰§æƒ…è¿è´¯æ€§")
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å‰§é›†")

    def get_srt_files(self) -> List[str]:
        """è·å–å­—å¹•æ–‡ä»¶åˆ—è¡¨"""
        if not os.path.exists(self.srt_folder):
            return []
        
        files = [f for f in os.listdir(self.srt_folder) if f.endswith('.srt')]
        files.sort()
        return files

    def get_video_files(self) -> List[str]:
        """è·å–è§†é¢‘æ–‡ä»¶åˆ—è¡¨"""
        if not os.path.exists(self.videos_folder):
            return []
        
        video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.wmv', '.flv']
        files = [f for f in os.listdir(self.videos_folder) 
                if any(f.lower().endswith(ext) for ext in video_extensions)]
        files.sort()
        return files

    def create_episode_report(self, analysis: Dict, created_clips: List[str]):
        """åˆ›å»ºå•é›†è¯¦ç»†æŠ¥å‘Š"""
        episode_number = analysis['episode_number']
        report_path = os.path.join(self.output_folder, f"E{episode_number}_è¯¦ç»†åˆ†ææŠ¥å‘Š.txt")
        
        content = f"""ğŸ“º ç¬¬{episode_number}é›† - æ™ºèƒ½åˆ†æä¸å‰ªè¾‘æŠ¥å‘Š
{'='*80}

ğŸ­ å‰§æƒ…åˆ†æ:
ä¸»è¦å‰§æƒ…çº¿: {analysis['episode_analysis'].get('main_storyline', 'æœªçŸ¥')}
æ ¸å¿ƒè§’è‰²: {', '.join(analysis['episode_analysis'].get('key_characters', []))}
æˆå‰§å…ƒç´ : {', '.join(analysis['episode_analysis'].get('dramatic_elements', []))}
æƒ…æ„ŸåŸºè°ƒ: {analysis['episode_analysis'].get('emotional_tone', 'æœªçŸ¥')}

ğŸ“– æœ¬é›†æ¦‚è¿°:
{analysis.get('episode_summary', 'æš‚æ— æ¦‚è¿°')}

ğŸ¬ ç²¾å½©ç‰‡æ®µè¯¦æƒ… ({len(analysis['clips'])} ä¸ª):
"""
        
        for i, clip in enumerate(analysis['clips'], 1):
            content += f"""
ç‰‡æ®µ {clip['clip_id']}: {clip['title']}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸ æ—¶é—´è½´: {clip['start_time']} --> {clip['end_time']} ({clip['duration']:.1f}ç§’)
ğŸ¯ å‰§æƒ…ä»·å€¼: {clip.get('significance', 'æœªçŸ¥')}
ğŸª å¸å¼•å–ç‚¹: {clip.get('hook_factor', 'æœªçŸ¥')}

ğŸ“– å™äº‹ç»“æ„:
  èƒŒæ™¯é“ºå«: {clip.get('narrative_structure', {}).get('setup', 'æš‚æ— ')}
  æ ¸å¿ƒå†²çª: {clip.get('narrative_structure', {}).get('conflict', 'æš‚æ— ')}
  é«˜æ½®éƒ¨åˆ†: {clip.get('narrative_structure', {}).get('climax', 'æš‚æ— ')}
  ç»“æœè§£å†³: {clip.get('narrative_structure', {}).get('resolution', 'æš‚æ— ')}

ğŸ™ï¸ ä¸“ä¸šè§£è¯´è¦ç‚¹:
  å¼€åœºå¼•å…¥: {clip.get('professional_commentary', {}).get('opening_hook', 'æš‚æ— ')}
  èƒŒæ™¯è§£é‡Š: {clip.get('professional_commentary', {}).get('background_context', 'æš‚æ— ')}
  å‰§æƒ…åˆ†æ: {clip.get('professional_commentary', {}).get('plot_analysis', 'æš‚æ— ')}
  æƒ…æ„Ÿå†²å‡»: {clip.get('professional_commentary', {}).get('emotional_impact', 'æš‚æ— ')}
  æ€»ç»“å‡å: {clip.get('professional_commentary', {}).get('conclusion', 'æš‚æ— ')}

ğŸ”— è¿æ¥ä¸‹ç‰‡æ®µ: {clip.get('connection_next', 'æš‚æ— ')}
"""
        
        content += f"""

ğŸ”„ è¿è´¯æ€§åˆ†æ:
{analysis.get('continuity_notes', 'æš‚æ— ')}

ğŸ“š å‰§é›†å®šä½:
{analysis.get('series_context', 'æš‚æ— ')}

ğŸ“Š æŠ€æœ¯ä¿¡æ¯:
â€¢ AIåˆ†æ: {'æ˜¯' if analysis.get('ai_generated') else 'å¦'}
â€¢ ç”Ÿæˆæ—¶é—´: {self.get_current_time()}
â€¢ è§†é¢‘æ–‡ä»¶: {len(created_clips)} ä¸ª
â€¢ ç¼“å­˜æ”¯æŒ: æ˜¯ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
"""
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"    ğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: E{episode_number}_è¯¦ç»†åˆ†ææŠ¥å‘Š.txt")
        except Exception as e:
            print(f"    âš ï¸ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

    def create_complete_report(self, all_results: List[Dict]):
        """åˆ›å»ºå®Œæ•´å‰§é›†è¿è´¯æ€§æŠ¥å‘Š"""
        report_path = os.path.join(self.output_folder, "å®Œæ•´å‰§é›†è¿è´¯æ€§æ€»ç»“.txt")
        
        total_clips = sum(len(r['clips']) for r in all_results)
        
        content = f"""ğŸ“º å®Œæ•´å‰§é›†æ™ºèƒ½åˆ†ææ€»ç»“æŠ¥å‘Š
{'='*80}
ç”Ÿæˆæ—¶é—´: {self.get_current_time()}
å¤„ç†é›†æ•°: {len(all_results)} é›†
æ€»çŸ­è§†é¢‘: {total_clips} ä¸ª
åˆ†ææ¨¡å¼: {'AIæ™ºèƒ½å¢å¼º' if self.ai_config.get('enabled') else 'è§„åˆ™åˆ†æ'}

ğŸ­ æ•´ä½“å‰§æƒ…å‘å±•è„‰ç»œ:
"""
        
        for result in all_results:
            analysis = result['analysis']
            content += f"""
ç¬¬{analysis['episode_number']}é›†: {analysis['episode_analysis'].get('main_storyline', 'æœªçŸ¥')}
â”œâ”€ ç²¾å½©ç‰‡æ®µ: {len(analysis['clips'])} ä¸ª
â”œâ”€ å‰§é›†å®šä½: {analysis.get('series_context', 'æš‚æ— ')}
â””â”€ è¿è´¯æ€§: {analysis.get('continuity_notes', 'æš‚æ— ')}
"""
        
        content += f"""

ğŸ“Š æ™ºèƒ½å‰ªè¾‘ç»Ÿè®¡:
â€¢ æ€»å¤„ç†é›†æ•°: {len(all_results)} é›†
â€¢ æ€»çŸ­è§†é¢‘æ•°: {total_clips} ä¸ª
â€¢ å¹³å‡æ¯é›†çŸ­è§†é¢‘: {total_clips / len(all_results):.1f} ä¸ª
â€¢ è¾“å‡ºç›®å½•: {self.output_folder}/
â€¢ æ—ç™½è§£è¯´æ–‡ä»¶: æ¯ä¸ªçŸ­è§†é¢‘é…å¥—
â€¢ ç¼“å­˜æœºåˆ¶: æ”¯æŒæ–­ç‚¹ç»­ä¼ 

ğŸ”„ è·¨é›†è¿è´¯æ€§ä¿è¯:
â€¢ æ‰€æœ‰ç‰‡æ®µæŒ‰æ—¶é—´é¡ºåºèƒ½å®Œæ•´å™è¿°å‰§æƒ…
â€¢ æ¯ä¸ªç‰‡æ®µåŒ…å«ä¸“ä¸šæ—ç™½è§£è¯´èƒŒæ™¯å’Œæ„ä¹‰
â€¢ AIåˆ†æè€ƒè™‘äº†å‰§æƒ…åè½¬å’Œä¼ç¬”çš„å‰åå…³è”
â€¢ ç¡®ä¿ä¸€å¥è¯è®²å®Œçš„å®Œæ•´æ€§
â€¢ æ™ºèƒ½è¯†åˆ«è€Œéå›ºå®šè§„åˆ™ï¼Œé€‚åº”å„ç§å‰§æƒ…ç±»å‹

ğŸ’¡ ä½¿ç”¨å»ºè®®:
â€¢ æ¯ä¸ªçŸ­è§†é¢‘éƒ½æœ‰å¯¹åº”çš„æ—ç™½è§£è¯´æ–‡ä»¶
â€¢ ç¼“å­˜ç³»ç»Ÿä¿è¯å¤šæ¬¡æ‰§è¡Œç»“æœä¸€è‡´
â€¢ æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå·²å¤„ç†å†…å®¹ä¸é‡å¤
â€¢ è§†é¢‘æ–‡ä»¶åœ¨ {self.output_folder}/ ç›®å½•
â€¢ æ—ç™½æ–‡ä»¶ä»¥ _æ—ç™½è§£è¯´.txt ç»“å°¾
"""
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Š: å®Œæ•´å‰§é›†è¿è´¯æ€§æ€»ç»“.txt")
        except Exception as e:
            print(f"ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå¤±è´¥: {e}")

    def get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´è½¬ç§’"""
        try:
            h, m, s_ms = time_str.split(':')
            s, ms = s_ms.split(',')
            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
        except:
            return 0

    def seconds_to_time(self, seconds: float) -> str:
        """ç§’è½¬æ—¶é—´"""
        try:
            h = int(seconds // 3600)
            m = int((seconds % 3600) // 60)
            s = int(seconds % 60)
            ms = int((seconds % 1) * 1000)
            return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
        except:
            return "00:00:00,000"

    def analyze_all_episodes(self) -> List[Dict]:
        """åˆ†ææ‰€æœ‰å‰§é›†ï¼ˆä»…åˆ†æï¼Œä¸å‰ªè¾‘ï¼‰"""
        # æ™ºèƒ½è¯†åˆ«å­—å¹•æ–‡ä»¶
        subtitle_files = []
        for file in os.listdir('.'):
            if file.endswith(('.txt', '.srt')) and not file.startswith('.'):
                # æ’é™¤ç³»ç»Ÿæ–‡ä»¶
                if any(keyword in file.lower() for keyword in ['readme', 'config', 'license']):
                    continue
                subtitle_files.append(file)

        subtitle_files.sort()

        if not subtitle_files:
            print("âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
            return []

        print(f"ğŸ“ å‘ç° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")

        all_results = []

        for filename in subtitle_files:
            print(f"\nğŸ” åˆ†æ: {filename}")
            subtitles = self.parse_subtitle_file(filename)

            if subtitles:
                result = self.ai_analyze_episode_content(subtitles, filename)
                all_results.append(result)

                print(f"âœ… {result['theme']}")
                print(f"   å‰§æƒ…ç±»å‹: {result['genre']}")
                print(f"   æ¨èç‰‡æ®µ: {len(result['clips'])} ä¸ª")
                if result['ai_analysis']:
                    print("   ğŸ¤– AIæ™ºèƒ½åˆ†æ")
                else:
                    print("   ğŸ“ å…³é”®è¯åˆ†æ")
            else:
                print(f"âŒ è§£æå¤±è´¥: {filename}")

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_analysis_report(all_results)

        return all_results

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶ - é€šç”¨æ ¼å¼æ”¯æŒ"""
        try:
            # å¤šç¼–ç å°è¯•
            for encoding in ['utf-8', 'gbk', 'gb2312']:
                try:
                    with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                        content = f.read()
                    break
                except:
                    continue

            # æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£
            corrections = {
                'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
                'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
                'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©',
                'è½è­‰æœƒ': 'å¬è¯ä¼š', 'è¾¯è­·': 'è¾©æŠ¤', 'å¯©åˆ¤': 'å®¡åˆ¤', 'èª¿æŸ¥': 'è°ƒæŸ¥'
            }

            for old, new in corrections.items():
                content = content.replace(old, new)

            # è§£æå­—å¹•å—
            import re
            blocks = re.split(r'\n\s*\n', content.strip())
            subtitles = []

            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    try:
                        index = int(lines[0])
                        time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                        if time_match:
                            start_time = time_match.group(1)
                            end_time = time_match.group(2)
                            text = '\n'.join(lines[2:])

                            subtitles.append({
                                'index': index,
                                'start': start_time,
                                'end': end_time,
                                'text': text,
                                'episode': os.path.basename(filepath)
                            })
                    except (ValueError, IndexError):
                        continue

            return subtitles
        except Exception as e:
            print(f"âŒ è§£æå­—å¹•æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return []

    def ai_analyze_episode_content(self, subtitles: List[Dict], episode_name: str) -> Dict:
        """AIæ™ºèƒ½åˆ†æå‰§é›†å†…å®¹"""
        if not self.ai_config.get('enabled'):
            return self.fallback_analysis_old(subtitles, episode_name)

        # æå–å…³é”®å¯¹è¯å†…å®¹
        key_dialogues = []
        for sub in subtitles[::10]:  # æ¯10æ¡å–1æ¡ï¼Œé¿å…è¿‡é•¿
            if len(sub['text']) > 10:
                key_dialogues.append(f"[{sub['start']}] {sub['text']}")

        content_sample = '\n'.join(key_dialogues[:50])  # æœ€å¤š50æ¡

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”µè§†å‰§ç‰‡æ®µå†…å®¹ï¼Œè¯†åˆ«æœ€ç²¾å½©çš„å‰§æƒ…ç‰‡æ®µç”¨äºçŸ­è§†é¢‘å‰ªè¾‘ã€‚

ã€å‰§é›†åç§°ã€‘: {episode_name}
ã€å¯¹è¯å†…å®¹ã€‘:
{content_sample}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ™ºèƒ½åˆ†æï¼š
1. å‰§æƒ…ç±»å‹è¯†åˆ«ï¼ˆæ³•å¾‹ã€çˆ±æƒ…ã€çŠ¯ç½ªã€å®¶åº­ã€å¤è£…ã€ç°ä»£ç­‰ï¼‰
2. æ ¸å¿ƒå†²çªç‚¹å’Œæˆå‰§å¼ åŠ›
3. æƒ…æ„Ÿé«˜æ½®æ—¶åˆ»
4. å…³é”®ä¿¡æ¯æ­éœ²ç‚¹
5. æ¨èçš„å‰ªè¾‘ç‰‡æ®µï¼ˆ2-3ä¸ªæœ€ç²¾å½©çš„æ—¶é—´æ®µï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
    "genre": "å‰§æƒ…ç±»å‹",
    "theme": "æœ¬é›†æ ¸å¿ƒä¸»é¢˜",
    "key_conflicts": ["å†²çª1", "å†²çª2"],
    "emotional_peaks": ["æƒ…æ„Ÿé«˜æ½®1", "æƒ…æ„Ÿé«˜æ½®2"],
    "recommended_clips": [
        {{
            "start_time": "å¼€å§‹æ—¶é—´",
            "end_time": "ç»“æŸæ—¶é—´", 
            "reason": "æ¨èç†ç”±",
            "content": "å†…å®¹æè¿°"
        }}
    ],
    "next_episode_hint": "ä¸ä¸‹é›†çš„è¡”æ¥ç‚¹"
}}
"""

        ai_response = self.call_ai_api(prompt)

        if ai_response:
            try:
                # æå–JSONéƒ¨åˆ†
                if "```json" in ai_response:
                    json_start = ai_response.find("```json") + 7
                    json_end = ai_response.find("```", json_start)
                    json_text = ai_response[json_start:json_end].strip()
                else:
                    start = ai_response.find("{")
                    end = ai_response.rfind("}") + 1
                    json_text = ai_response[start:end]

                result = json.loads(json_text)
                return self.process_ai_analysis_old(result, subtitles, episode_name)

            except Exception as e:
                print(f"âš  AIå›å¤è§£æå¤±è´¥: {e}")
                return self.fallback_analysis_old(subtitles, episode_name)

        return self.fallback_analysis_old(subtitles, episode_name)

    def process_ai_analysis_old(self, ai_result: Dict, subtitles: List[Dict], episode_name: str) -> Dict:
        """å¤„ç†AIåˆ†æç»“æœ"""
        clips = []

        for rec_clip in ai_result.get('recommended_clips', []):
            start_time = rec_clip.get('start_time')
            end_time = rec_clip.get('end_time')

            if start_time and end_time:
                clips.append({
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': self.time_to_seconds(end_time) - self.time_to_seconds(start_time),
                    'reason': rec_clip.get('reason', ''),
                    'content': rec_clip.get('content', ''),
                    'ai_recommended': True
                })

        # æå–é›†æ•°
        import re
        episode_match = re.search(r'[Ee](\d+)', episode_name)
        episode_num = episode_match.group(1) if episode_match else "00"

        return {
            'episode': episode_name,
            'episode_number': episode_num,
            'theme': f"E{episode_num}: {ai_result.get('theme', 'ç²¾å½©ç‰‡æ®µ')}",
            'genre': ai_result.get('genre', 'æœªçŸ¥'),
            'clips': clips,
            'key_conflicts': ai_result.get('key_conflicts', []),
            'emotional_peaks': ai_result.get('emotional_peaks', []),
            'next_episode_hint': ai_result.get('next_episode_hint', ''),
            'ai_analysis': True
        }

    def fallback_analysis_old(self, subtitles: List[Dict], episode_name: str) -> Dict:
        """å¤‡ç”¨åˆ†æï¼ˆæ— AIæ—¶ï¼‰"""
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ
        dramatic_keywords = [
            'çªç„¶', 'å‘ç°', 'çœŸç›¸', 'ç§˜å¯†', 'ä¸å¯èƒ½', 'ä¸ºä»€ä¹ˆ', 'æ€äºº', 'æ­»äº†', 
            'æ•‘å‘½', 'å±é™©', 'å®Œäº†', 'éœ‡æƒŠ', 'æ„¤æ€’', 'å“­', 'å´©æºƒ'
        ]

        high_score_segments = []

        for i, subtitle in enumerate(subtitles):
            score = 0
            text = subtitle['text']

            # å…³é”®è¯è¯„åˆ†
            for keyword in dramatic_keywords:
                if keyword in text:
                    score += 2

            # æ ‡ç‚¹ç¬¦å·è¯„åˆ†
            score += text.count('ï¼') + text.count('ï¼Ÿ') + text.count('...') * 0.5

            if score >= 3:
                high_score_segments.append({
                    'index': i,
                    'subtitle': subtitle,
                    'score': score
                })

        # é€‰æ‹©æœ€ä½³ç‰‡æ®µ
        high_score_segments.sort(key=lambda x: x['score'], reverse=True)

        clips = []
        for seg in high_score_segments[:3]:  # æœ€å¤š3ä¸ªç‰‡æ®µ
            start_idx = max(0, seg['index'] - 10)
            end_idx = min(len(subtitles) - 1, seg['index'] + 15)

            clips.append({
                'start_time': subtitles[start_idx]['start'],
                'end_time': subtitles[end_idx]['end'],
                'duration': self.time_to_seconds(subtitles[end_idx]['end']) - self.time_to_seconds(subtitles[start_idx]['start']),
                'reason': 'åŸºäºå…³é”®è¯è¯†åˆ«çš„ç²¾å½©ç‰‡æ®µ',
                'content': seg['subtitle']['text'],
                'ai_recommended': False
            })

        import re
        episode_match = re.search(r'[Ee](\d+)', episode_name)
        episode_num = episode_match.group(1) if episode_match else "00"

        return {
            'episode': episode_name,
            'episode_number': episode_num,
            'theme': f"E{episode_num}: ç²¾å½©ç‰‡æ®µåˆé›†",
            'genre': 'é€šç”¨',
            'clips': clips,
            'key_conflicts': ['å‰§æƒ…å†²çª'],
            'emotional_peaks': ['æƒ…æ„Ÿé«˜æ½®'],
            'next_episode_hint': 'æ•…äº‹ç»§ç»­å‘å±•',
            'ai_analysis': False
        }

    def generate_analysis_report(self, results: List[Dict]):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not results:
            return

        content = "ğŸ“º æ™ºèƒ½ç”µè§†å‰§åˆ†ææŠ¥å‘Š\n"
        content += "=" * 80 + "\n\n"

        if self.ai_config.get('enabled'):
            content += f"ğŸ¤– AIåˆ†ææ¨¡å¼: {self.ai_config['model']}\n"
        else:
            content += "ğŸ“ å…³é”®è¯åˆ†ææ¨¡å¼\n"

        content += f"ğŸ“Š åˆ†æé›†æ•°: {len(results)} é›†\n\n"

        total_clips = 0
        for result in results:
            content += f"ğŸ“º {result['theme']}\n"
            content += "-" * 60 + "\n"
            content += f"å‰§æƒ…ç±»å‹: {result['genre']}\n"
            content += f"æ¨èç‰‡æ®µ: {len(result['clips'])} ä¸ª\n"

            for i, clip in enumerate(result['clips'], 1):
                content += f"\nğŸ¬ ç‰‡æ®µ {i}:\n"
                content += f"   æ—¶é—´: {clip['start_time']} --> {clip['end_time']}\n"
                content += f"   æ—¶é•¿: {clip['duration']:.1f} ç§’\n"
                content += f"   æ¨èç†ç”±: {clip['reason']}\n"
                content += f"   å†…å®¹: {clip['content'][:50]}...\n"

            if result['key_conflicts']:
                content += f"\nğŸ’¥ æ ¸å¿ƒå†²çª: {', '.join(result['key_conflicts'])}\n"

            if result['emotional_peaks']:
                content += f"ğŸ˜Š æƒ…æ„Ÿé«˜æ½®: {', '.join(result['emotional_peaks'])}\n"

            content += f"ğŸ”— ä¸‹é›†è¡”æ¥: {result['next_episode_hint']}\n"
            content += "=" * 80 + "\n\n"

            total_clips += len(result['clips'])

        content += f"ğŸ“Š æ€»è®¡æ¨èç‰‡æ®µ: {total_clips} ä¸ª\n"

        with open('æ™ºèƒ½åˆ†ææŠ¥å‘Š.txt', 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: æ™ºèƒ½åˆ†ææŠ¥å‘Š.txt")

    def main_menu(self):
        """ä¸»èœå•"""
        while True:
            print("\n" + "=" * 60)
            print("ğŸ“º æ™ºèƒ½ç”µè§†å‰§åˆ†æå‰ªè¾‘ç³»ç»Ÿ")
            print("=" * 60)

            ai_status = "ğŸ¤– AIå¢å¼º" if self.ai_config.get('enabled') else "ğŸ“ å…³é”®è¯åˆ†æ"
            print(f"å½“å‰æ¨¡å¼: {ai_status}")

            if self.ai_config.get('enabled'):
                print(f"AIæ¨¡å‹: {self.ai_config['model']}")

            print("è¯·é€‰æ‹©æ“ä½œ:")
            print("1. ğŸ“ æ™ºèƒ½åˆ†æå­—å¹•")
            print("2. ğŸ¬ é«˜çº§æ™ºèƒ½å‰ªè¾‘ (æ¨è)")
            print("3. ğŸ¤– é…ç½®AIæ¥å£")
            print("4. ğŸ“Š æŸ¥çœ‹åˆ†ææŠ¥å‘Š")
            print("5. âŒ é€€å‡º")

            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()

                if choice == '1':
                    self.analyze_all_episodes()

                elif choice == '2':
                    # é«˜çº§æ™ºèƒ½å‰ªè¾‘ - æ•´åˆæ‰€æœ‰æ”¹è¿›åŠŸèƒ½
                    self.run_advanced_clipping()

                elif choice == '3':
                    self.setup_ai_config()

                elif choice == '4':
                    # æŸ¥çœ‹å¤šç§æŠ¥å‘Š
                    reports = [
                        'æ™ºèƒ½åˆ†ææŠ¥å‘Š.txt',
                        f'{self.output_folder}/å®Œæ•´å‰§é›†è¿è´¯æ€§æ€»ç»“.txt',
                        'smart_analysis_report.txt'
                    ]

                    found_report = False
                    for report_file in reports:
                        if os.path.exists(report_file):
                            with open(report_file, 'r', encoding='utf-8') as f:
                                print(f"\nğŸ“„ {report_file}:")
                                content = f.read()
                                print(content[:1500] + "..." if len(content) > 1500 else content)
                                found_report = True
                                break

                    if not found_report:
                        print("âŒ æœªæ‰¾åˆ°åˆ†ææŠ¥å‘Šï¼Œè¯·å…ˆæ‰§è¡Œåˆ†æ")

                elif choice == '5':
                    print("\nğŸ‘‹ å†è§ï¼")
                    break

                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")
                break

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")

    # æ£€æŸ¥å­—å¹•æ–‡ä»¶
    subtitle_files = []
    for file in os.listdir('.'):
        if file.endswith(('.txt', '.srt')) and not file.startswith('.'):
            if not any(keyword in file.lower() for keyword in ['readme', 'config', 'license']):
                subtitle_files.append(file)

    if subtitle_files:
        print(f"âœ… æ‰¾åˆ° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")
        return True
    else:
        print("âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
        print("è¯·ç¡®ä¿å­—å¹•æ–‡ä»¶(.txt/.srt)åœ¨å½“å‰ç›®å½•")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½ç”µè§†å‰§åˆ†æå‰ªè¾‘ç³»ç»Ÿå¯åŠ¨")

    if not check_environment():
        input("\næŒ‰Enteré”®é€€å‡º...")
        return

    try:
        system = AIClipperSystem()
        system.main_menu()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        input("\næŒ‰Enteré”®é€€å‡º...")

if __name__ == "__main__":
    main()