#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½å­—å¹•åˆ†æå™¨ - ç¡®ä¿å‰§æƒ…è¿è´¯æ€§å’Œåè½¬å…³è”
"""

import os
import re
import json
import requests
from typing import List, Dict, Tuple, Optional
from datetime import datetime

class SubtitleAnalyzer:
    """æ™ºèƒ½å­—å¹•åˆ†æå™¨ - ä¸“æ³¨äºå‰§æƒ…è¿è´¯æ€§"""

    def __init__(self, ai_config: Dict):
        self.ai_config = ai_config
        self.enabled = ai_config.get('enabled', False) if ai_config else False

        # å‰§æƒ…è¿è´¯æ€§å…³é”®è¯
        self.plot_continuity_keywords = {
            'å‰æƒ…å›é¡¾': ['ä¹‹å‰', 'åˆšæ‰', 'å½“æ—¶', 'é‚£æ—¶å€™', 'ä¸Šæ¬¡', 'æ—©äº›æ—¶å€™'],
            'æƒ…èŠ‚æ¨è¿›': ['æ¥ç€', 'ç„¶å', 'éšå', 'åæ¥', 'æ¥ä¸‹æ¥', 'ç°åœ¨'],
            'åè½¬é“ºå«': ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸è¿‡', 'å…¶å®', 'åŸæ¥', 'æ²¡æƒ³åˆ°'],
            'é‡è¦æ­éœ²': ['çœŸç›¸', 'ç§˜å¯†', 'å‘ç°', 'è¯æ®', 'çº¿ç´¢', 'å…³é”®'],
            'æƒ…æ„Ÿè½¬æŠ˜': ['çªç„¶', 'å¿½ç„¶', 'æ„å¤–', 'éœ‡æƒŠ', 'æƒŠè®¶', 'æ²¡æ–™åˆ°'],
            'è§’è‰²å‘å±•': ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜', 'æˆé•¿', 'è§‰æ‚Ÿ', 'æ˜ç™½']
        }

        # åè½¬æƒ…èŠ‚æ ‡è¯†
        self.plot_twist_indicators = [
            'åŸæ¥', 'å…¶å®', 'æ²¡æƒ³åˆ°', 'ç«Ÿç„¶', 'å±…ç„¶', 'äº‹å®ä¸Š',
            'çœŸç›¸æ˜¯', 'å®é™…ä¸Š', 'ä¸æ˜¯', 'è€Œæ˜¯', 'åè€Œ', 'ç›¸å'
        ]

        # å‰§æƒ…å…³è”è¯
        self.story_connection_words = [
            'å› ä¸º', 'æ‰€ä»¥', 'å¯¼è‡´', 'ç»“æœ', 'å¼•èµ·', 'é€ æˆ',
            'ç”±äº', 'åŸºäº', 'æ ¹æ®', 'æŒ‰ç…§', 'ä¾æ®', 'è€ƒè™‘åˆ°'
        ]

        # é”™åˆ«å­—ä¿®æ­£
        self.corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
            'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©'
        }

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶å¹¶ä¿®æ­£é”™åˆ«å­—"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # ä¿®æ­£é”™åˆ«å­—
            for old, new in self.corrections.items():
                content = content.replace(old, new)

            # è§£æå­—å¹•æ ¼å¼
            blocks = re.split(r'\n\s*\n', content.strip())
            subtitles = []

            for block in blocks:
                lines = block.strip().split('\n')
                if len(lines) >= 3:
                    try:
                        index = int(lines[0])
                        time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                        if time_match:
                            start_time = time_match.group(1)
                            end_time = time_match.group(2)
                            text = '\n'.join(lines[2:]).strip()

                            subtitles.append({
                                'index': index,
                                'start': start_time,
                                'end': end_time,
                                'text': text,
                                'start_seconds': self._time_to_seconds(start_time),
                                'end_seconds': self._time_to_seconds(end_time)
                            })
                    except (ValueError, IndexError):
                        continue

            return subtitles

        except Exception as e:
            print(f"  è§£æå­—å¹•æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´å­—ç¬¦ä¸²è½¬ç§’æ•°"""
        try:
            h, m, s_ms = time_str.split(':')
            s, ms = s_ms.split(',')
            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
        except:
            return 0.0

    def analyze_episode(self, file_path: str) -> Dict:
        """åˆ†æå•é›†ï¼Œç¡®ä¿å‰§æƒ…è¿è´¯æ€§"""
        filename = os.path.basename(file_path)
        print(f"ğŸ” åˆ†ææ–‡ä»¶: {filename}")

        # æ£€æŸ¥ç¼“å­˜
        cache_name = os.path.splitext(filename)[0] + '.json'
        cache_path = os.path.join('analysis_cache', cache_name)

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_result = json.load(f)
                print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return cached_result
            except:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œé‡æ–°åˆ†æ")

        # è§£æå­—å¹•
        subtitles = self.parse_subtitle_file(file_path)
        if not subtitles:
            return {'clips': [], 'episode': filename}

        print(f"  ğŸ“„ è§£æå®Œæˆ: {len(subtitles)} æ¡å­—å¹•")

        # æ™ºèƒ½ç‰‡æ®µåˆ†æ
        clips = self._analyze_coherent_clips(subtitles, filename)

        # æ„å»ºç»“æœ
        result = {
            'episode': filename,
            'clips': clips,
            'total_clips': len(clips),
            'analysis_time': datetime.now().isoformat(),
            'continuity_analysis': self._analyze_story_continuity(clips),
            'plot_connections': self._find_plot_connections(clips)
        }

        # ä¿å­˜ç¼“å­˜
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ç»“æœå·²ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        return result

    def _analyze_coherent_clips(self, subtitles: List[Dict], filename: str) -> List[Dict]:
        """åˆ†æè¿è´¯çš„å‰§æƒ…ç‰‡æ®µ"""
        clips = []

        # è®¡ç®—å­—å¹•é‡è¦æ€§è¯„åˆ†
        scored_subtitles = []
        for i, subtitle in enumerate(subtitles):
            score = self._calculate_importance_score(subtitle['text'], i, len(subtitles))
            if score >= 3.0:  # é‡è¦åº¦é˜ˆå€¼
                scored_subtitles.append({
                    'index': i,
                    'subtitle': subtitle,
                    'score': score
                })

        # æŒ‰è¯„åˆ†æ’åº
        scored_subtitles.sort(key=lambda x: x['score'], reverse=True)

        # æ™ºèƒ½åˆå¹¶è¿è´¯ç‰‡æ®µ
        used_indices = set()

        for scored_sub in scored_subtitles[:5]:  # æœ€å¤š5ä¸ªæ ¸å¿ƒç‰‡æ®µ
            center_index = scored_sub['index']

            if center_index in used_indices:
                continue

            # å¯»æ‰¾è¿è´¯çš„ç‰‡æ®µèŒƒå›´
            clip_range = self._find_coherent_range(subtitles, center_index)
            start_idx, end_idx = clip_range

            # æ£€æŸ¥æ—¶é•¿åˆç†æ€§
            duration = subtitles[end_idx]['end_seconds'] - subtitles[start_idx]['start_seconds']
            if 60 <= duration <= 180:  # 1-3åˆ†é’Ÿ

                clip = {
                    'start_time': subtitles[start_idx]['start'],
                    'end_time': subtitles[end_idx]['end'],
                    'duration': duration,
                    'score': scored_sub['score'],
                    'description': self._generate_clip_description(subtitles, start_idx, end_idx),
                    'theme': self._extract_clip_theme(subtitles, start_idx, end_idx),
                    'key_dialogues': self._extract_key_dialogues(subtitles, start_idx, end_idx),
                    'plot_significance': self._analyze_plot_significance(subtitles, start_idx, end_idx),
                    'continuity_markers': self._find_continuity_markers(subtitles, start_idx, end_idx),
                    'twist_potential': self._detect_twist_potential(subtitles, start_idx, end_idx)
                }

                clips.append(clip)

                # æ ‡è®°å·²ä½¿ç”¨çš„ç´¢å¼•
                for idx in range(start_idx, end_idx + 1):
                    used_indices.add(idx)

        # æŒ‰æ—¶é—´æ’åºï¼Œç¡®ä¿æ•…äº‹é¡ºåº
        clips.sort(key=lambda x: self._time_to_seconds(x['start_time']))

        return clips

    def _calculate_importance_score(self, text: str, position: int, total: int) -> float:
        """è®¡ç®—å­—å¹•é‡è¦æ€§è¯„åˆ†"""
        score = 0.0

        # åŸºç¡€é•¿åº¦è¯„åˆ†
        if 10 <= len(text) <= 100:
            score += 1.0

        # å‰§æƒ…è¿è´¯æ€§è¯„åˆ†
        for category, keywords in self.plot_continuity_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    if category == 'é‡è¦æ­éœ²':
                        score += 3.0
                    elif category == 'åè½¬é“ºå«':
                        score += 2.5
                    elif category == 'æƒ…èŠ‚æ¨è¿›':
                        score += 2.0
                    else:
                        score += 1.5

        # åè½¬æƒ…èŠ‚è¯„åˆ†
        for twist_word in self.plot_twist_indicators:
            if twist_word in text:
                score += 3.0

        # æƒ…æ„Ÿå¼ºåº¦è¯„åˆ†
        emotion_indicators = ['ï¼', 'ï¼Ÿ', '...', 'å•Š', 'å“¦', 'å“‡', 'å¤©å“ª']
        for indicator in emotion_indicators:
            score += text.count(indicator) * 0.5

        # ä½ç½®æƒé‡
        position_ratio = position / total
        if 0.2 <= position_ratio <= 0.8:  # ä¸­é—´éƒ¨åˆ†æ›´é‡è¦
            score *= 1.2

        return score

    def _find_coherent_range(self, subtitles: List[Dict], center_index: int) -> Tuple[int, int]:
        """å¯»æ‰¾è¿è´¯çš„ç‰‡æ®µèŒƒå›´"""
        start_idx = center_index
        end_idx = center_index

        # å‘å‰æ‰©å±•
        for i in range(center_index - 1, max(0, center_index - 20), -1):
            if self._is_scene_break(subtitles[i]['text'], subtitles[i + 1]['text']):
                break
            start_idx = i

        # å‘åæ‰©å±•
        for i in range(center_index + 1, min(len(subtitles), center_index + 20)):
            if self._is_scene_break(subtitles[i - 1]['text'], subtitles[i]['text']):
                break
            end_idx = i

        return start_idx, end_idx

    def _is_scene_break(self, prev_text: str, current_text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåœºæ™¯åˆ‡æ¢"""
        scene_break_indicators = [
            'é•œå¤´åˆ‡æ¢', 'åœºæ™¯è½¬æ¢', 'åŒæ—¶', 'å¦ä¸€è¾¹', 'æ­¤æ—¶',
            'ç”»é¢ä¸€è½¬', 'è½¬çœ¼é—´', 'çªç„¶é—´'
        ]

        for indicator in scene_break_indicators:
            if indicator in current_text:
                return True

        # æ–‡æœ¬é•¿åº¦å·®å¼‚è¿‡å¤§
        if abs(len(prev_text) - len(current_text)) > 50:
            return True

        return False

    def _generate_clip_description(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """ç”Ÿæˆç‰‡æ®µæè¿°"""
        key_texts = []
        for i in range(start_idx, min(end_idx + 1, start_idx + 3)):
            text = subtitles[i]['text']
            if len(text) > 10:
                key_texts.append(text[:30] + "..." if len(text) > 30 else text)

        return " | ".join(key_texts) if key_texts else "ç²¾å½©ç‰‡æ®µ"

    def _extract_clip_theme(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """æå–ç‰‡æ®µä¸»é¢˜"""
        full_text = " ".join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        # ä¸»é¢˜å…³é”®è¯æ£€æµ‹
        themes = {
            'æ³•å¾‹äº‰è®®': ['æ³•åº­', 'å®¡åˆ¤', 'å¾‹å¸ˆ', 'æ£€å¯Ÿå®˜', 'è¯æ®', 'è¾©æŠ¤'],
            'æ¡ˆä»¶è°ƒæŸ¥': ['è°ƒæŸ¥', 'çº¿ç´¢', 'çœŸç›¸', 'ç ´æ¡ˆ', 'ç–‘çŠ¯', 'æ¡ˆä»¶'],
            'æƒ…æ„Ÿå†²çª': ['æ„¤æ€’', 'ç—›è‹¦', 'æ‚²ä¼¤', 'äº‰åµ', 'åˆ†æ­§', 'çŸ›ç›¾'],
            'çœŸç›¸æ­éœ²': ['å‘ç°', 'çœŸç›¸', 'ç§˜å¯†', 'åŸæ¥', 'å…¶å®', 'æ²¡æƒ³åˆ°'],
            'è§’è‰²æˆé•¿': ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜', 'æˆé•¿', 'è§‰æ‚Ÿ', 'åšæŒ']
        }

        theme_scores = {}
        for theme, keywords in themes.items():
            score = sum(1 for keyword in keywords if keyword in full_text)
            if score > 0:
                theme_scores[theme] = score

        if theme_scores:
            return max(theme_scores, key=theme_scores.get)

        return "å‰§æƒ…å‘å±•"

    def _extract_key_dialogues(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> List[str]:
        """æå–å…³é”®å¯¹è¯"""
        key_dialogues = []

        for i in range(start_idx, end_idx + 1):
            text = subtitles[i]['text']
            start_time = subtitles[i]['start']
            end_time = subtitles[i]['end']

            # åˆ¤æ–­æ˜¯å¦ä¸ºå…³é”®å¯¹è¯
            is_key = False
            if any(word in text for word in self.plot_twist_indicators):
                is_key = True
            elif any(word in text for word in ['çœŸç›¸', 'è¯æ®', 'å‘ç°', 'ç§˜å¯†']):
                is_key = True
            elif '!' in text or 'ï¼Ÿ' in text:
                is_key = True

            if is_key and len(text) > 5:
                key_dialogues.append(f"[{start_time} --> {end_time}] {text}")

        return key_dialogues[:5]  # æœ€å¤š5æ¡å…³é”®å¯¹è¯

    def _analyze_plot_significance(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """åˆ†æå‰§æƒ…æ„ä¹‰"""
        full_text = " ".join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        significance_points = []

        # æ£€æµ‹å‰§æƒ…å‘å±•ç±»å‹
        if any(word in full_text for word in ['è¯æ®', 'çº¿ç´¢', 'å‘ç°']):
            significance_points.append("é‡è¦è¯æ®æŠ«éœ²")

        if any(word in full_text for word in ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜']):
            significance_points.append("è§’è‰²å‘å±•è½¬æŠ˜")

        if any(word in full_text for word in self.plot_twist_indicators):
            significance_points.append("å‰§æƒ…åè½¬å…³é”®")

        if any(word in full_text for word in ['å†²çª', 'äº‰è®º', 'å¯¹æŠ—']):
            significance_points.append("æˆå‰§å†²çªé«˜æ½®")

        if any(word in full_text for word in ['çœŸç›¸', 'ç§˜å¯†', 'æ­éœ²']):
            significance_points.append("çœŸç›¸æ­ç¤ºæ—¶åˆ»")

        return "ã€".join(significance_points) if significance_points else "é‡è¦å‰§æƒ…èŠ‚ç‚¹"

    def _find_continuity_markers(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> List[str]:
        """å¯»æ‰¾è¿è´¯æ€§æ ‡è®°"""
        markers = []
        full_text = " ".join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        for category, keywords in self.plot_continuity_keywords.items():
            for keyword in keywords:
                if keyword in full_text:
                    markers.append(f"{category}:{keyword}")

        return markers

    def _detect_twist_potential(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> float:
        """æ£€æµ‹åè½¬æ½œåŠ›"""
        full_text = " ".join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        twist_score = 0.0
        for twist_word in self.plot_twist_indicators:
            twist_score += full_text.count(twist_word) * 1.0

        # å½’ä¸€åŒ–åˆ°0-1
        return min(twist_score / 5.0, 1.0)

    def _analyze_story_continuity(self, clips: List[Dict]) -> Dict:
        """åˆ†ææ•…äº‹è¿è´¯æ€§"""
        if not clips:
            return {'continuity_score': 0, 'connections': []}

        connections = []
        total_score = 0

        for i in range(len(clips) - 1):
            current_clip = clips[i]
            next_clip = clips[i + 1]

            # åˆ†æç‰‡æ®µé—´çš„è¿æ¥å¼ºåº¦
            connection_strength = self._calculate_connection_strength(current_clip, next_clip)
            total_score += connection_strength

            connections.append({
                'from_clip': i,
                'to_clip': i + 1,
                'strength': connection_strength,
                'connection_type': self._identify_connection_type(current_clip, next_clip)
            })

        avg_score = total_score / len(connections) if connections else 0

        return {
            'continuity_score': avg_score,
            'connections': connections,
            'overall_coherence': 'high' if avg_score > 0.7 else 'medium' if avg_score > 0.4 else 'low'
        }

    def _calculate_connection_strength(self, clip1: Dict, clip2: Dict) -> float:
        """è®¡ç®—ç‰‡æ®µé—´è¿æ¥å¼ºåº¦"""
        strength = 0.0

        # ä¸»é¢˜è¿ç»­æ€§
        if clip1['theme'] == clip2['theme']:
            strength += 0.3

        # æ—¶é—´é—´éš”
        time1 = self._time_to_seconds(clip1['end_time'])
        time2 = self._time_to_seconds(clip2['start_time'])
        time_gap = time2 - time1

        if time_gap < 300:  # 5åˆ†é’Ÿå†…
            strength += 0.4
        elif time_gap < 900:  # 15åˆ†é’Ÿå†…
            strength += 0.2

        # åè½¬å…³è”
        if clip1['twist_potential'] > 0.5 and clip2['twist_potential'] > 0.5:
            strength += 0.3

        return min(strength, 1.0)

    def _identify_connection_type(self, clip1: Dict, clip2: Dict) -> str:
        """è¯†åˆ«è¿æ¥ç±»å‹"""
        if 'çœŸç›¸æ­éœ²' in clip1['plot_significance'] and 'çœŸç›¸æ­éœ²' in clip2['plot_significance']:
            return 'çœŸç›¸é€’è¿›'
        elif 'åè½¬' in clip1['plot_significance'] or 'åè½¬' in clip2['plot_significance']:
            return 'æƒ…èŠ‚åè½¬'
        elif clip1['theme'] == clip2['theme']:
            return 'ä¸»é¢˜å»¶ç»­'
        else:
            return 'æƒ…èŠ‚æ¨è¿›'

    def _find_plot_connections(self, clips: List[Dict]) -> List[Dict]:
        """å¯»æ‰¾å‰§æƒ…å…³è”ç‚¹"""
        connections = []

        for i, clip in enumerate(clips):
            # å¯»æ‰¾ä¸å…¶ä»–ç‰‡æ®µçš„å…³è”
            for j, other_clip in enumerate(clips):
                if i != j:
                    connection = self._analyze_clip_connection(clip, other_clip, i, j)
                    if connection:
                        connections.append(connection)

        return connections

    def _analyze_clip_connection(self, clip1: Dict, clip2: Dict, idx1: int, idx2: int) -> Optional[Dict]:
        """åˆ†æä¸¤ä¸ªç‰‡æ®µçš„å…³è”"""
        # æ£€æŸ¥å…³é”®è¯é‡å 
        desc1_words = set(clip1['description'].split())
        desc2_words = set(clip2['description'].split())
        common_words = desc1_words & desc2_words

        if len(common_words) >= 2:  # è‡³å°‘2ä¸ªå…±åŒå…³é”®è¯
            return {
                'clip1_index': idx1,
                'clip2_index': idx2,
                'connection_type': 'keyword_overlap',
                'common_elements': list(common_words),
                'strength': len(common_words) / max(len(desc1_words), len(desc2_words))
            }

        # æ£€æŸ¥åè½¬å…³è”
        if clip1['twist_potential'] > 0.5 and clip2['twist_potential'] > 0.5:
            return {
                'clip1_index': idx1,
                'clip2_index': idx2,
                'connection_type': 'plot_twist_chain',
                'common_elements': ['åè½¬æƒ…èŠ‚'],
                'strength': (clip1['twist_potential'] + clip2['twist_potential']) / 2
            }

        return None

# The following part of the code was in the original file,
# but it's not present in the edited snippet. To ensure a complete
# and working file, I'm adding it back based on the intention.
from clip_rules import ClipRules

# å¹³å°å…¼å®¹æ€§ä¿®å¤
try:
    from platform_fix import fix_encoding, safe_file_read, safe_file_write
    fix_encoding()
except ImportError:
    def safe_file_read(filepath, encoding='utf-8'):
        with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
            return f.read()

    def safe_file_write(filepath, content, encoding='utf-8'):
        with open(filepath, 'w', encoding=encoding, errors='ignore') as f:
            f.write(content)

class IntelligentSubtitleAnalyzer:
    def __init__(self, use_ai_analysis: bool = True, api_key: Optional[str] = None):
        self.use_ai_analysis = use_ai_analysis
        self.api_key = api_key

        # æ™ºèƒ½å‰§æƒ…ç±»å‹è¯†åˆ«
        self.genre_patterns = {
            'legal': {
                'keywords': ['æ³•å®˜', 'æ£€å¯Ÿå®˜', 'å¾‹å¸ˆ', 'æ³•åº­', 'å®¡åˆ¤', 'è¯æ®', 'æ¡ˆä»¶', 'èµ·è¯‰', 'è¾©æŠ¤', 'åˆ¤å†³', 'ç”³è¯‰', 'å¬è¯ä¼š'],
                'weight': 1.0
            },
            'crime': {
                'keywords': ['è­¦å¯Ÿ', 'çŠ¯ç½ª', 'å«Œç–‘äºº', 'è°ƒæŸ¥', 'ç ´æ¡ˆ', 'çº¿ç´¢', 'å‡¶æ‰‹', 'æ¡ˆå‘', 'ä¾¦æ¢', 'åˆ‘ä¾¦', 'è¿½è¸ª', 'é€®æ•'],
                'weight': 1.0
            },
            'medical': {
                'keywords': ['åŒ»ç”Ÿ', 'æŠ¤å£«', 'åŒ»é™¢', 'æ‰‹æœ¯', 'ç—…äºº', 'è¯Šæ–­', 'æ²»ç–—', 'ç—…æƒ…', 'æ€¥è¯Š', 'æ•‘æŠ¤è½¦', 'è¯ç‰©', 'ç—…æˆ¿'],
                'weight': 1.0
            },
            'romance': {
                'keywords': ['çˆ±æƒ…', 'å–œæ¬¢', 'å¿ƒåŠ¨', 'è¡¨ç™½', 'çº¦ä¼š', 'åˆ†æ‰‹', 'å¤åˆ', 'ç»“å©š', 'æƒ…ä¾£', 'æ‹äºº', 'æš—æ‹', 'åˆæ‹'],
                'weight': 1.0
            },
            'family': {
                'keywords': ['å®¶åº­', 'çˆ¶æ¯', 'å­©å­', 'å…„å¼Ÿ', 'å§å¦¹', 'äº²æƒ…', 'å®¶äºº', 'å›¢èš', 'ç¦»åˆ«', 'æˆé•¿', 'æ•™è‚²', 'ä»£æ²Ÿ'],
                'weight': 1.0
            },
            'business': {
                'keywords': ['å…¬å¸', 'è€æ¿', 'å‘˜å·¥', 'åˆä½œ', 'ç«äº‰', 'é¡¹ç›®', 'ä¼šè®®', 'è°ˆåˆ¤', 'æŠ•èµ„', 'åˆ›ä¸š', 'èŒåœº', 'æ™‹å‡'],
                'weight': 1.0
            },
            'historical': {
                'keywords': ['çš‡å¸', 'å¤§è‡£', 'æœå»·', 'æˆ˜äº‰', 'å°†å†›', 'å£«å…µ', 'ç‹æœ', 'å®«å»·', 'æ”¿æ²»', 'æƒåŠ›', 'å›ä¹±', 'èµ·ä¹‰'],
                'weight': 1.0
            },
            'fantasy': {
                'keywords': ['é­”æ³•', 'æ­¦åŠŸ', 'ä¿®ç‚¼', 'ä»™äºº', 'å¦–æ€ª', 'ç¥è¯', 'ä¼ è¯´', 'æ³•æœ¯', 'çµåŠ›', 'å¼‚èƒ½', 'ç©¿è¶Š', 'é‡ç”Ÿ'],
                'weight': 1.0
            }
        }

        # é€šç”¨æˆå‰§å¼ åŠ›æ ‡è¯†è¯
        self.universal_dramatic_keywords = [
            'çªç„¶', 'å¿½ç„¶', 'æ²¡æƒ³åˆ°', 'åŸæ¥', 'å±…ç„¶', 'ç«Ÿç„¶', 'éœ‡æƒŠ', 'æƒŠè®¶', 'æ„å¤–', 'å‘ç°',
            'çœŸç›¸', 'ç§˜å¯†', 'éšç’', 'æ­éœ²', 'æš´éœ²', 'çˆ†å‘', 'å†²çª', 'å¯¹æŠ—', 'äº‰åµ', 'åè½¬',
            'å±é™©', 'ç´§æ€¥', 'æ•‘å‘½', 'å¸®åŠ©', 'æ‹¯æ•‘', 'é€ƒè·‘', 'è¿½é€', 'æ‰“æ–—', 'ç”Ÿæ­»', 'å…³é”®'
        ]

        # é€šç”¨æƒ…æ„Ÿçˆ†å‘ç‚¹æ ‡è¯†
        self.universal_emotional_markers = [
            'å“­', 'ç¬‘', 'å–Š', 'å«', 'æ€’', 'æ°”', 'æ¿€åŠ¨', 'å…´å¥‹', 'ç´§å¼ ', 'å®³æ€•', 'æ‹…å¿ƒ', 'ç„¦è™‘',
            'å¼€å¿ƒ', 'é«˜å…´', 'æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'å¸Œæœ›', 'æ„ŸåŠ¨', 'æ¸©æš–', 'å¤±æœ›',
            'æ„¤æ€’', 'ç”Ÿæ°”', 'æš´æ€’', 'å´©æºƒ', 'é¢¤æŠ–', 'å¿ƒè·³', 'å‘¼å¸', 'çœ¼æ³ª', 'å¾®ç¬‘', 'æ‹¥æŠ±'
        ]

        # å¯¹è¯å¼ºåº¦æ ‡è¯†
        self.dialogue_intensity_markers = [
            'ï¼', 'ï¼Ÿ', '...', 'å•Š', 'å‘€', 'å“¦', 'å”‰', 'å“', 'å—¯', 'å‘¢', 'å§', 'å—', 'å’¦', 'å“‡'
        ]

        # åŠ¨æ€è°ƒæ•´æƒé‡
        self.adaptive_weights = {
            'genre_match': 5.0,
            'dramatic_tension': 3.0,
            'emotional_intensity': 2.0,
            'dialogue_richness': 1.5,
            'character_development': 2.5,
            'plot_advancement': 4.0
        }

        # è‡ªåŠ¨è¯†åˆ«çš„å‰§æƒ…ç±»å‹
        self.detected_genre = None
        self.genre_confidence = 0.0

    def detect_genre(self, all_subtitles: List[Dict]) -> str:
        """æ™ºèƒ½è¯†åˆ«å‰§æƒ…ç±»å‹"""
        genre_scores = {}
        total_text = " ".join([sub['text'] for sub in all_subtitles[:200]])  # åˆ†æå‰200æ¡å­—å¹•

        for genre, pattern in self.genre_patterns.items():
            score = 0
            for keyword in pattern['keywords']:
                score += total_text.count(keyword) * pattern['weight']
            genre_scores[genre] = score

        # æ‰¾å‡ºæœ€åŒ¹é…çš„ç±»å‹
        if genre_scores:
            best_genre = max(genre_scores, key=genre_scores.get)
            max_score = genre_scores[best_genre]

            if max_score > 5:  # è¶³å¤Ÿçš„åŒ¹é…åº¦
                self.detected_genre = best_genre
                self.genre_confidence = min(max_score / 50, 1.0)  # å½’ä¸€åŒ–ä¿¡å¿ƒåº¦
                return best_genre

        return 'general'  # é€šç”¨ç±»å‹

    def get_genre_specific_keywords(self, genre: str) -> List[str]:
        """è·å–ç‰¹å®šç±»å‹çš„å…³é”®è¯"""
        if genre in self.genre_patterns:
            return self.genre_patterns[genre]['keywords']
        return []

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶ï¼Œè‡ªåŠ¨ä¿®æ­£å¸¸è§é”™åˆ«å­—"""
        content = safe_file_read(filepath)

        # é€šç”¨é”™åˆ«å­—ä¿®æ­£
        corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'è¨­è¨ˆ': 'è®¾è®¡', 'é–‹å§‹': 'å¼€å§‹', 'çµæŸ': 'ç»“æŸ',
            'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©'
        }

        for old, new in corrections.items():
            content = content.replace(old, new)

        # åˆ†å‰²å­—å¹•å—
        blocks = re.split(r'\n\s*\n', content.strip())
        subtitles = []

        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0])
                    time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                    if time_match:
                        start_time = time_match.group(1)
                        end_time = time_match.group(2)
                        text = '\n'.join(lines[2:])

                        subtitles.append({
                            'index': index,
                            'start': start_time,
                            'end': end_time,
                            'text': text,
                            'episode': os.path.basename(filepath)
                        })
                except (ValueError, IndexError):
                    continue

        return subtitles

    def calculate_intelligent_score(self, text: str, context: Dict = None) -> float:
        """æ™ºèƒ½è¯„åˆ†ç®—æ³• - è‡ªé€‚åº”ä¸åŒå‰§æƒ…ç±»å‹"""

        # åŸºç¡€è§„åˆ™è¯„åˆ†
        rule_score = self._calculate_adaptive_rule_score(text, context)

        # å¦‚æœå¯ç”¨AIåˆ†æï¼Œè·å–AIè¯„åˆ†
        if self.use_ai_analysis:
            ai_score = self._calculate_ai_score(text, context)
            if ai_score > 0:
                # åŠ¨æ€æ··åˆè¯„åˆ†
                rule_weight = 0.4 if self.genre_confidence > 0.7 else 0.6
                ai_weight = 1 - rule_weight
                final_score = rule_score * rule_weight + ai_score * ai_weight
                return final_score

        return rule_score

    def _calculate_adaptive_rule_score(self, text: str, context: Dict = None) -> float:
        """è‡ªé€‚åº”è§„åˆ™è¯„åˆ†"""
        score = 0

        # ç±»å‹åŒ¹é…è¯„åˆ†
        if self.detected_genre:
            genre_keywords = self.get_genre_specific_keywords(self.detected_genre)
            for keyword in genre_keywords:
                if keyword in text:
                    score += self.adaptive_weights['genre_match'] * self.genre_confidence

        # é€šç”¨æˆå‰§å¼ åŠ›è¯„åˆ†
        for keyword in self.universal_dramatic_keywords:
            if keyword in text:
                score += self.adaptive_weights['dramatic_tension']

        # æƒ…æ„Ÿå¼ºåº¦è¯„åˆ†
        for emotion in self.universal_emotional_markers:
            if emotion in text:
                score += self.adaptive_weights['emotional_intensity']

        # å¯¹è¯ä¸°å¯Œåº¦
        for marker in self.dialogue_intensity_markers:
            score += text.count(marker) * self.adaptive_weights['dialogue_richness']

        # æ–‡æœ¬é•¿åº¦å’Œè´¨é‡
        text_length = len(text)
        if 15 <= text_length <= 200:
            score += 2
        elif text_length > 200:
            score += 1

        # è§’è‰²å‘å±•æ ‡è¯†
        character_indicators = ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜', 'æˆé•¿', 'é¢†æ‚Ÿ', 'æ˜ç™½', 'åšæŒ', 'æ”¾å¼ƒ']
        for indicator in character_indicators:
            if indicator in text:
                score += self.adaptive_weights['character_development']

        # å‰§æƒ…æ¨è¿›æ ‡è¯†
        plot_indicators = ['ç„¶å', 'æ¥ç€', 'éšå', 'çªç„¶', 'è¿™æ—¶', 'ç»“æœ', 'æœ€ç»ˆ', 'ç»ˆäº']
        for indicator in plot_indicators:
            if indicator in text:
                score += self.adaptive_weights['plot_advancement']

        return score

    def _calculate_ai_score(self, text: str, context: Dict = None) -> float:
        """ä½¿ç”¨AIæ¨¡å‹è¯„ä¼°å‰§æƒ…é‡è¦æ€§ - æ”¯æŒå¤šç§æ¨¡å‹"""
        try:
            from api_config_helper import config_helper
            
            config = config_helper.load_config()
            if not config.get('enabled') or not config.get('api_key'):
                return 0.0

            if len(text) < 10:
                return 0.0
            if len(text) > 500:
                text = text[:500] + "..."

            # åŠ¨æ€ç”Ÿæˆæç¤ºè¯
            genre_context = f"è¿™æ˜¯ä¸€éƒ¨{self.detected_genre}ç±»å‹çš„ç”µè§†å‰§" if self.detected_genre else "è¿™æ˜¯ä¸€éƒ¨ç”µè§†å‰§"

            prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯†åˆ«ç²¾å½©ç‰‡æ®µã€‚

{genre_context}ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å¯¹è¯ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ï¼š
"{text}"

è¯„ä¼°æ ‡å‡†ï¼š
1. å‰§æƒ…é‡è¦æ€§(0-2åˆ†)ï¼šæ˜¯å¦æ¨è¿›ä¸»è¦æ•…äº‹çº¿ï¼ŒåŒ…å«å…³é”®ä¿¡æ¯
2. æˆå‰§å¼ åŠ›(0-2åˆ†)ï¼šæ˜¯å¦åŒ…å«å†²çªã€è½¬æŠ˜ã€æ„å¤–å‘ç°
3. æƒ…æ„Ÿå…±é¸£(0-2åˆ†)ï¼šæ˜¯å¦å¼•å‘è§‚ä¼—æƒ…æ„Ÿååº”ï¼Œæœ‰æ„Ÿäººæˆ–éœ‡æ’¼æ—¶åˆ»
4. è§’è‰²å‘å±•(0-2åˆ†)ï¼šæ˜¯å¦å±•ç°è§’è‰²æˆé•¿ã€å…³ç³»å˜åŒ–ã€é‡è¦å†³å®š
5. è§‚ä¼—å¸å¼•åŠ›(0-2åˆ†)ï¼šæ˜¯å¦åˆ¶é€ æ‚¬å¿µã€å¹½é»˜ã€ç´§å¼ æ„Ÿ

è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†ç»™å‡º0-10åˆ†çš„ç»¼åˆè¯„åˆ†ã€‚
åªéœ€è¦è¾“å‡ºæ•°å­—ï¼Œä¾‹å¦‚ï¼š8.5"""

            # æ ¹æ®ä¸åŒAPIæä¾›å•†æ„å»ºè¯·æ±‚
            provider = config.get('provider', 'openai')
            
            if provider == 'gemini':
                return self._call_gemini_api(config, prompt)
            elif provider == 'qwen':
                return self._call_qwen_api(config, prompt)
            elif provider == 'doubao':
                return self._call_doubao_api(config, prompt)
            else:
                return self._call_standard_api(config, prompt)

        except Exception as e:
            return 0.0

    def _call_gemini_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨Gemini API"""
        try:
            url = f"https://generativelanguage.googleapis.com/v1/models/{config['model']}:generateContent?key={config['api_key']}"
            
            data = {
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "maxOutputTokens": 20,
                    "temperature": 0.2
                }
            }
            
            response = requests.post(url, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                score_match = re.search(r'(\d+\.?\d*)', text.strip())
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_qwen_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'input': {'prompt': prompt},
                'parameters': {
                    'max_tokens': 20,
                    'temperature': 0.2
                }
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('output', {}).get('text', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_doubao_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨è±†åŒ…API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯„ä¼°ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†ç»™å‡º0-10åˆ†çš„æ•°å­—è¯„åˆ†ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.2
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            
            return 0.0
            
        except Exception:
            return 0.0

    def _call_standard_api(self, config: Dict, prompt: str) -> float:
        """è°ƒç”¨æ ‡å‡†OpenAIæ ¼å¼API"""
        try:
            headers = {
                'Authorization': f'Bearer {config["api_key"]}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': config['model'],
                'messages': [
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆï¼Œä¸“æ³¨äºè¯„ä¼°ç‰‡æ®µçš„å‰ªè¾‘ä»·å€¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†ç»™å‡º0-10åˆ†çš„æ•°å­—è¯„åˆ†ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.2
            }
            
            response = requests.post(config['url'], headers=headers, json=data, timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('choices', [{}])[0].get('message', {}).get('content', '').strip()
                score_match = re.search(r'(\d+\.?\d*)', text)
                if score_match:
                    return min(max(float(score_match.group(1)), 0), 10)
            else:
                print(f"âš  APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            
            return 0.0
            
        except requests.exceptions.RequestException as e:
            print(f"âš  ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return 0.0
        except Exception as e:
            print(f"âš  APIè°ƒç”¨å¼‚å¸¸: {e}")
            return 0.0

    def identify_smart_segments(self, subtitles: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½è¯†åˆ«ç²¾å½©ç‰‡æ®µ"""
        high_relevance_segments = []

        # æ™ºèƒ½é˜ˆå€¼è°ƒæ•´ - é’ˆå¯¹çŸ­è§†é¢‘ä¼˜åŒ–
        base_threshold = 5.0  # é™ä½åŸºç¡€é˜ˆå€¼ï¼Œè·å–æ›´å¤šç²¾å½©ç‰‡æ®µ
        if self.detected_genre in ['crime', 'legal']:
            threshold = base_threshold - 1.5  # æ³•å¾‹å‰§ç²¾å½©ç‰‡æ®µè¾ƒå¤š
        elif self.detected_genre in ['romance', 'family']:
            threshold = base_threshold - 1.0  # æƒ…æ„Ÿå‰§ä¹Ÿéœ€è¦æ›´å¤šç‰‡æ®µ
        else:
            threshold = base_threshold

        for i, subtitle in enumerate(subtitles):
            # æ„å»ºä¸Šä¸‹æ–‡
            context = {
                'position': i / len(subtitles),  # åœ¨å‰§é›†ä¸­çš„ä½ç½®
                'prev_text': subtitles[i-1]['text'] if i > 0 else '',
                'next_text': subtitles[i+1]['text'] if i < len(subtitles)-1 else '',
                'genre': self.detected_genre
            }

            score = self.calculate_intelligent_score(subtitle['text'], context)
            if score >= threshold:
                high_relevance_segments.append({
                    'index': i,
                    'subtitle': subtitle,
                    'score': score,
                    'context': context
                })

        # æŒ‰åˆ†æ•°æ’åº
        high_relevance_segments.sort(key=lambda x: x['score'], reverse=True)

        # æ™ºèƒ½ç‰‡æ®µåˆå¹¶
        coherent_segments = self._merge_coherent_segments(subtitles, high_relevance_segments)

        return coherent_segments

    def _merge_coherent_segments(self, subtitles: List[Dict], high_segments: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½åˆå¹¶è¿è´¯ç‰‡æ®µ"""
        coherent_segments = []
        used_indices = set()

        for segment_data in high_segments:
            start_idx = segment_data['index']
            if start_idx in used_indices:
                continue

            # åŠ¨æ€æ‰©å±•èŒƒå›´
            expand_range = 12 if self.detected_genre in ['romance', 'family'] else 10

            segment_start = max(0, start_idx - expand_range)
            segment_end = min(len(subtitles) - 1, start_idx + expand_range)

            # å¯»æ‰¾è‡ªç„¶è¾¹ç•Œ
            segment_start = self._find_natural_start(subtitles, segment_start, start_idx)
            segment_end = self._find_natural_end(subtitles, start_idx, segment_end)

            # è®¡ç®—æ—¶é•¿
            start_time = self.time_to_seconds(subtitles[segment_start]['start'])
            end_time = self.time_to_seconds(subtitles[segment_end]['end'])
            duration = end_time - start_time

            # åŠ¨æ€æ—¶é•¿æ§åˆ¶
            min_duration = 90 if self.detected_genre in ['romance', 'family'] else 120
            max_duration = 200 if self.detected_genre in ['crime', 'legal'] else 180

            if min_duration <= duration <= max_duration:
                coherent_segments.append({
                    'start_index': segment_start,
                    'end_index': segment_end,
                    'start_time': subtitles[segment_start]['start'],
                    'end_time': subtitles[segment_end]['end'],
                    'duration': duration,
                    'core_content': self._extract_smart_content(subtitles, segment_start, segment_end),
                    'key_dialogue': self._extract_key_dialogue(subtitles, segment_start, segment_end),
                    'significance': self._analyze_smart_significance(subtitles, segment_start, segment_end),
                    'emotional_tone': self._analyze_emotional_tone(subtitles, segment_start, segment_end),
                    'genre': self.detected_genre,
                    'confidence': segment_data['score']
                })

                # æ ‡è®°å·²ä½¿ç”¨çš„ç´¢å¼•
                for idx in range(segment_start, segment_end + 1):
                    used_indices.add(idx)

        return coherent_segments[:3]  # æ¯é›†æœ€å¤š3ä¸ªæ ¸å¿ƒç‰‡æ®µ

    def _find_natural_start(self, subtitles: List[Dict], search_start: int, anchor: int) -> int:
        """å¯»æ‰¾è‡ªç„¶å¼€å§‹ç‚¹"""
        scene_starters = ['çªç„¶', 'è¿™æ—¶', 'å¿½ç„¶', 'åˆšæ‰', 'ç°åœ¨', 'é‚£å¤©', 'å½“æ—¶', 'éšå³', 'åæ¥', 'æ¥ç€']

        for i in range(anchor, search_start - 1, -1):
            text = subtitles[i]['text']
            if any(starter in text for starter in scene_starters):
                return i
            if len(text) < 8 and '...' in text:
                return i + 1

        return search_start

    def _find_natural_end(self, subtitles: List[Dict], anchor: int, search_end: int) -> int:
        """å¯»æ‰¾è‡ªç„¶ç»“æŸç‚¹"""
        scene_enders = ['èµ°äº†', 'ç¦»å¼€', 'ç»“æŸ', 'ç®—äº†', 'å¥½å§', 'å†è§', 'æ‹œæ‹œ', 'å®Œäº†', 'è¡Œäº†']

        for i in range(anchor, search_end + 1):
            text = subtitles[i]['text']
            if any(ender in text for ender in scene_enders):
                return i
            if '...' in text and i > anchor + 8:
                return i

        return search_end

    def _extract_smart_content(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """æ™ºèƒ½æå–æ ¸å¿ƒå†…å®¹"""
        key_texts = []
        genre_keywords = self.get_genre_specific_keywords(self.detected_genre) if self.detected_genre else []

        for i in range(start_idx, min(end_idx + 1, start_idx + 6)):
            text = subtitles[i]['text']

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
            has_key_info = False
            if genre_keywords:
                has_key_info = any(keyword in text for keyword in genre_keywords)

            if has_key_info or any(keyword in text for keyword in self.universal_dramatic_keywords):
                key_texts.append(text.strip())

        return ' | '.join(key_texts) if key_texts else subtitles[start_idx]['text']

    def _extract_key_dialogue(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> List[str]:
        """æå–å…³é”®å¯¹è¯"""
        key_dialogues = []

        for i in range(start_idx, end_idx + 1):
            text = subtitles[i]['text'].strip()
            score = self.calculate_intelligent_score(text)

            if score >= 4.0:
                time_code = f"{subtitles[i]['start']} --> {subtitles[i]['end']}"
                key_dialogues.append(f"[{time_code}] {text}")

        return key_dialogues[:5]

    def _analyze_smart_significance(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """æ™ºèƒ½åˆ†æå‰§æƒ…æ„ä¹‰"""
        content = ' '.join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])
        significance_points = []

        # æ ¹æ®å‰§æƒ…ç±»å‹åˆ†æ
        if self.detected_genre == 'legal':
            if any(word in content for word in ['æ¡ˆä»¶', 'è¯æ®', 'å®¡åˆ¤']):
                significance_points.append("æ³•å¾‹ç¨‹åºæ¨è¿›")
        elif self.detected_genre == 'romance':
            if any(word in content for word in ['è¡¨ç™½', 'çº¦ä¼š', 'åˆ†æ‰‹']):
                significance_points.append("æƒ…æ„Ÿå…³ç³»å‘å±•")
        elif self.detected_genre == 'crime':
            if any(word in content for word in ['çº¿ç´¢', 'ç ´æ¡ˆ', 'çœŸç›¸']):
                significance_points.append("æ¡ˆä»¶è°ƒæŸ¥è¿›å±•")
        elif self.detected_genre == 'family':
            if any(word in content for word in ['å®¶åº­', 'äº²æƒ…', 'æˆé•¿']):
                significance_points.append("å®¶åº­å…³ç³»å‘å±•")

        # é€šç”¨æ„ä¹‰åˆ†æ
        if any(word in content for word in ['å‘ç°', 'çœŸç›¸', 'ç§˜å¯†']):
            significance_points.append("é‡è¦ä¿¡æ¯æ­éœ²")
        if any(word in content for word in ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜']):
            significance_points.append("è§’è‰²å‘å±•è½¬æŠ˜")
        if any(word in content for word in ['å†²çª', 'äº‰è®º', 'å¯¹æŠ—']):
            significance_points.append("æˆå‰§å†²çªé«˜æ½®")

        return "ã€".join(significance_points) if significance_points else "å‰§æƒ…é‡è¦èŠ‚ç‚¹"

    def _analyze_emotional_tone(self, subtitles: List[Dict], start_idx: int, end_idx: int) -> str:
        """åˆ†ææƒ…æ„ŸåŸºè°ƒ"""
        content = ' '.join([subtitles[i]['text'] for i in range(start_idx, end_idx + 1)])

        emotion_scores = {
            'positive': sum(1 for word in ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å¹¸ç¦', 'æ¸©æš–', 'æ„ŸåŠ¨'] if word in content),
            'negative': sum(1 for word in ['æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'æ„¤æ€’', 'ææƒ§'] if word in content),
            'tense': sum(1 for word in ['ç´§å¼ ', 'å±é™©', 'æ€¥', 'å¿«', 'å†²çª', 'äº‰è®º'] if word in content),
            'romantic': sum(1 for word in ['çˆ±', 'å–œæ¬¢', 'å¿ƒåŠ¨', 'æµªæ¼«', 'ç”œèœœ'] if word in content)
        }

        dominant_emotion = max(emotion_scores, key=emotion_scores.get)
        return dominant_emotion if emotion_scores[dominant_emotion] > 0 else 'neutral'

    def generate_smart_summary(self, episode_file: str, segments: List[Dict]) -> Dict:
        """ç”Ÿæˆæ™ºèƒ½å‰§é›†æ‘˜è¦"""
        episode_num = re.search(r'(\d+)', episode_file)
        episode_number = episode_num.group(1) if episode_num else "00"

        best_segments = segments[:2] if len(segments) >= 2 else segments
        total_duration = sum(seg['duration'] for seg in best_segments)

        # æ™ºèƒ½ç”Ÿæˆä¸»é¢˜
        if self.detected_genre:
            genre_desc = {
                'legal': 'æ³•å¾‹äº‰è®®',
                'romance': 'æƒ…æ„Ÿçº è‘›',
                'crime': 'æ¡ˆä»¶è°ƒæŸ¥',
                'family': 'å®¶åº­æ¸©æƒ…',
                'medical': 'åŒ»ç–—æ•‘æ²»',
                'business': 'å•†æˆ˜é£äº‘',
                'historical': 'å†å²å˜è¿',
                'fantasy': 'å¥‡å¹»å†’é™©'
            }.get(self.detected_genre, 'ç²¾å½©ç‰‡æ®µ')
        else:
            genre_desc = 'ç²¾å½©ç‰‡æ®µ'

        # ä»æ ¸å¿ƒå†…å®¹æå–å…³é”®ä¿¡æ¯
        key_elements = []
        for seg in best_segments:
            content = seg['core_content']
            if len(content) > 20:
                key_elements.append(content.split(' | ')[0][:15])

        theme_desc = 'ã€'.join(key_elements[:2]) if key_elements else genre_desc
        theme = f"E{episode_number}ï¼š{theme_desc}"

        return {
            'episode': episode_file,
            'episode_number': episode_number,
            'theme': theme,
            'genre': self.detected_genre or 'general',
            'genre_confidence': self.genre_confidence,
            'total_duration': total_duration,
            'segments': best_segments,
            'highlights': self._extract_smart_highlights(best_segments),
            'emotional_journey': self._analyze_emotional_journey(best_segments),
            'content_summary': self._generate_smart_content_summary(best_segments)
        }

    def _extract_smart_highlights(self, segments: List[Dict]) -> List[str]:
        """æ™ºèƒ½æå–äº®ç‚¹"""
        highlights = []

        for seg in segments:
            content_preview = seg['core_content'][:40] + "..." if len(seg['core_content']) > 40 else seg['core_content']

            # æ ¹æ®å‰§æƒ…ç±»å‹è°ƒæ•´äº®ç‚¹æè¿°
            if self.detected_genre == 'legal':
                value_desc = "æ³•å¾‹å†²çªæ ¸å¿ƒåœºé¢"
            elif self.detected_genre == 'romance':
                value_desc = "æƒ…æ„Ÿé«˜æ½®å…³é”®æ—¶åˆ»"
            elif self.detected_genre == 'crime':
                value_desc = "æ¡ˆä»¶ç ´è§£é‡è¦çº¿ç´¢"
            else:
                value_desc = "å‰§æƒ…å…³é”®è½¬æŠ˜ç‚¹"

            highlights.append(f"â€¢ {seg['significance']}ï¼š{content_preview} ({value_desc})")

        return highlights

    def _analyze_emotional_journey(self, segments: List[Dict]) -> str:
        """åˆ†ææƒ…æ„Ÿå†ç¨‹"""
        emotions = [seg.get('emotional_tone', 'neutral') for seg in segments]
        return " â†’ ".join(emotions)

    def _generate_smart_content_summary(self, segments: List[Dict]) -> str:
        """ç”Ÿæˆæ™ºèƒ½å†…å®¹æ‘˜è¦"""
        if not segments:
            return "æš‚æ— é‡ç‚¹å†…å®¹"

        significances = [seg['significance'] for seg in segments]
        return "ã€".join(set(significances))

    def time_to_seconds(self, time_str: str) -> float:
        """æ—¶é—´è½¬æ¢"""
        h, m, s_ms = time_str.split(':')
        s, ms = s_ms.split(',')
        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000

    def analyze_single_episode(self, episode_file: str) -> Dict:
        """åˆ†æå•é›†"""
        print(f"ğŸ” æ™ºèƒ½åˆ†æ {episode_file}...")

        subtitles = self.parse_subtitle_file(episode_file)

        # é¦–æ¬¡åˆ†ææ—¶æ£€æµ‹å‰§æƒ…ç±»å‹
        if self.detected_genre is None:
            detected_genre = self.detect_genre(subtitles)
            print(f"ğŸ­ æ£€æµ‹åˆ°å‰§æƒ…ç±»å‹: {detected_genre} (ç½®ä¿¡åº¦: {self.genre_confidence:.2f})")

        core_segments = self.identify_smart_segments(subtitles)
        episode_summary = self.generate_smart_summary(episode_file, core_segments)

        # æ·»åŠ è¿è´¯æ€§åˆ†æ
        episode_summary['continuity_hints'] = self._analyze_episode_continuity(core_segments)
        episode_summary['next_episode_connection'] = self._generate_next_episode_hint(core_segments)

        return episode_summary

    def _analyze_episode_continuity(self, segments: List[Dict]) -> List[str]:
        """åˆ†æé›†å†…è¿è´¯æ€§æç¤º"""
        hints = []
        for i, segment in enumerate(segments):
            content = segment['core_content']

            # åˆ†ææƒ…èŠ‚æ¨è¿›çº¿ç´¢
            if 'æ¡ˆä»¶' in content or 'è¯æ®' in content:
                hints.append(f"ç‰‡æ®µ{i+1}: æ¡ˆä»¶è°ƒæŸ¥å…³é”®è¿›å±•")
            elif 'æ³•åº­' in content or 'å®¡åˆ¤' in content:
                hints.append(f"ç‰‡æ®µ{i+1}: æ³•åº­è¾©è®ºæ ¸å¿ƒåœºé¢")
            elif any(word in content for word in ['çœŸç›¸', 'å‘ç°', 'ç§˜å¯†']):
                hints.append(f"ç‰‡æ®µ{i+1}: é‡è¦ä¿¡æ¯æ­éœ²æ—¶åˆ»")
            elif any(word in content for word in ['å†³å®š', 'é€‰æ‹©', 'æ”¹å˜']):
                hints.append(f"ç‰‡æ®µ{i+1}: è§’è‰²å‘å±•è½¬æŠ˜ç‚¹")

        return hints

    def _generate_next_episode_hint(self, segments: List[Dict]) -> str:
        """ç”Ÿæˆä¸ä¸‹ä¸€é›†çš„è¡”æ¥æç¤º"""
        if not segments:
            return "æœ¬é›†ä¸ºç‹¬ç«‹æƒ…èŠ‚ï¼Œæ•¬è¯·æœŸå¾…ä¸‹é›†ç²¾å½©"

        last_segment = segments[-1]
        content = last_segment['core_content']

        # æ ¹æ®ç»“å°¾å†…å®¹é¢„æµ‹ä¸‹é›†æ–¹å‘
        if 'ç»§ç»­' in content or 'ä¸‹æ¬¡' in content:
            return "æœ¬é›†åŸ‹ä¸‹ä¼ç¬”ï¼Œä¸‹é›†å°†æœ‰é‡å¤§è¿›å±•"
        elif 'å¬è¯ä¼š' in content:
            return "å¬è¯ä¼šå‡†å¤‡å°±ç»ªï¼Œä¸‹é›†æ³•åº­æ¿€è¾©å³å°†å¼€å§‹"
        elif 'ç”³è¯‰' in content or 'é‡å®¡' in content:
            return "ç”³è¯‰ç¨‹åºå¯åŠ¨ï¼Œä¸‹é›†æ¡ˆä»¶è¿æ¥è½¬æœº"
        elif 'è¯æ®' in content:
            return "å…³é”®è¯æ®æµ®ç°ï¼Œä¸‹é›†çœŸç›¸å³å°†å¤§ç™½"
        elif 'å±é™©' in content or 'å¨èƒ' in content:
            return "å±æœºå‡çº§ï¼Œä¸‹é›†æƒ…å†µæ›´åŠ ç´§æ€¥"
        else:
            return "å‰§æƒ…æŒç»­å‘å±•ï¼Œä¸‹é›†æ›´å¤šç²¾å½©ç­‰æ‚¨è§‚çœ‹"

def analyze_all_episodes_intelligently():
    """æ™ºèƒ½åˆ†ææ•´ä¸ªå‰§é›†"""
    analyzer = IntelligentSubtitleAnalyzer()

    # è·å–æ‰€æœ‰å­—å¹•æ–‡ä»¶
    subtitle_files = [f for f in os.listdir('.') if f.endswith('.txt') and ('E' in f or 'e' in f)]
    subtitle_files.sort()

    if not subtitle_files:
        print("âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶")
        return []

    print(f"ğŸ“º æ™ºèƒ½åˆ†æç³»ç»Ÿå¯åŠ¨ - å‘ç° {len(subtitle_files)} ä¸ªå­—å¹•æ–‡ä»¶")
    print("ğŸ¤– ç³»ç»Ÿç‰¹æ€§ï¼š")
    print("â€¢ è‡ªåŠ¨è¯†åˆ«å‰§æƒ…ç±»å‹ï¼ˆæ³•å¾‹/çˆ±æƒ…/çŠ¯ç½ª/å®¶åº­/åŒ»ç–—/å•†æˆ˜/å¤è£…/å¥‡å¹»ï¼‰")
    print("â€¢ åŠ¨æ€è°ƒæ•´è¯„åˆ†æƒé‡å’Œæ—¶é•¿æ§åˆ¶")
    print("â€¢ AIè¾…åŠ©åˆ†ææå‡å‡†ç¡®æ€§")
    print("â€¢ æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£å’Œç‰‡æ®µåˆå¹¶")
    print("=" * 60)

    all_episodes_plans = []

    for filename in subtitle_files:
        try:
            episode_plan = analyzer.analyze_single_episode(filename)
            all_episodes_plans.append(episode_plan)

            print(f"âœ“ {filename}")
            print(f"  ğŸ­ ç±»å‹: {episode_plan.get('genre', 'unknown')}")
            print(f"  ğŸ“ ä¸»é¢˜: {episode_plan['theme']}")
            print(f"  â±ï¸ æ—¶é•¿: {episode_plan['total_duration']:.1f}ç§’")
            print(f"  ğŸ¯ å†…å®¹: {episode_plan['content_summary']}")
            print(f"  ğŸ˜Š æƒ…æ„Ÿ: {episode_plan.get('emotional_journey', 'neutral')}")
            print()

        except Exception as e:
            print(f"âœ— é”™è¯¯å¤„ç† {filename}: {e}")

    # ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š
    generate_intelligent_report(all_episodes_plans, analyzer)

    return all_episodes_plans

def generate_intelligent_report(episodes_plans: List[Dict], analyzer):
    """ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š"""
    if not episodes_plans:
        return

    content = "ğŸ“º æ™ºèƒ½ç”µè§†å‰§å‰ªè¾‘åˆ†ææŠ¥å‘Š\n"
    content += "=" * 80 + "\n\n"

    # ç³»ç»Ÿä¿¡æ¯
    content += "ğŸ¤– æ™ºèƒ½åˆ†æç³»ç»Ÿä¿¡æ¯ï¼š\n"
    content += f"â€¢ æ£€æµ‹åˆ°çš„å‰§æƒ…ç±»å‹: {analyzer.detected_genre or 'é€šç”¨'}\n"
    content += f"â€¢ ç±»å‹è¯†åˆ«ç½®ä¿¡åº¦: {analyzer.genre_confidence:.2f}\n"
    content += f"â€¢ åˆ†ææ¨¡å¼: {'AIå¢å¼º' if analyzer.use_ai_analysis else 'è§„åˆ™åˆ†æ'}\n"
    content += f"â€¢ æ€»é›†æ•°: {len(episodes_plans)} é›†\n\n"

    # å‰§æƒ…ç±»å‹ç»Ÿè®¡
    genre_stats = {}
    for plan in episodes_plans:
        genre = plan.get('genre', 'unknown')
        genre_stats[genre] = genre_stats.get(genre, 0) + 1

    content += "ğŸ“Š å‰§æƒ…ç±»å‹åˆ†å¸ƒï¼š\n"
    for genre, count in genre_stats.items():
        content += f"â€¢ {genre}: {count} é›†\n"
    content += "\n"

    # è¯¦ç»†åˆ†æ
    total_duration = 0
    for i, plan in enumerate(episodes_plans):
        content += f"ğŸ“º {plan['theme']}\n"
        content += "-" * 60 + "\n"
        content += f"å‰§æƒ…ç±»å‹ï¼š{plan.get('genre', 'unknown')}\n"
        content += f"æ€»æ—¶é•¿ï¼š{plan['total_duration']:.1f} ç§’ ({plan['total_duration']/60:.1f} åˆ†é’Ÿ)\n"
        content += f"æƒ…æ„Ÿå†ç¨‹ï¼š{plan.get('emotional_journey', 'neutral')}\n"
        content += f"ç‰‡æ®µæ•°ï¼š{len(plan['segments'])} ä¸ªæ ¸å¿ƒç‰‡æ®µ\n\n"

        # ç‰‡æ®µè¯¦æƒ…
        for j, segment in enumerate(plan['segments']):
            content += f"ğŸ¬ ç‰‡æ®µ {j+1}ï¼š\n"
            content += f"   æ—¶é—´ï¼š{segment['start_time']} --> {segment['end_time']}\n"
            content += f"   æ—¶é•¿ï¼š{segment['duration']:.1f} ç§’\n"
            content += f"   æ„ä¹‰ï¼š{segment['significance']}\n"
            content += f"   æƒ…æ„Ÿï¼š{segment.get('emotional_tone', 'neutral')}\n"
            content += f"   ç½®ä¿¡åº¦ï¼š{segment.get('confidence', 0):.1f}\n"
            content += f"   å†…å®¹ï¼š{segment['core_content'][:100]}...\n"

            if segment['key_dialogue']:
                content += "   å…³é”®å¯¹è¯ï¼š\n"
                for dialogue in segment['key_dialogue'][:3]:
                    content += f"     {dialogue}\n"
            content += "\n"

        # äº®ç‚¹
        if plan['highlights']:
            content += "âœ¨ æœ¬é›†äº®ç‚¹ï¼š\n"
            for highlight in plan['highlights']:
                content += f"   {highlight}\n"

        total_duration += plan['total_duration']
        content += "=" * 80 + "\n\n"

    # æ€»ç»“
    content += f"ğŸ“Š æ™ºèƒ½åˆ†ææ€»ç»“ï¼š\n"
    content += f"â€¢ åˆ†æé›†æ•°ï¼š{len(episodes_plans)} é›†\n"
    content += f"â€¢ æ€»æ—¶é•¿ï¼š{total_duration:.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)\n"
    content += f"â€¢ å¹³å‡æ¯é›†ï¼š{total_duration/len(episodes_plans):.1f} ç§’\n"
    content += f"â€¢ ä¸»è¦ç±»å‹ï¼š{analyzer.detected_genre or 'é€šç”¨'}\n"
    content += f"â€¢ åˆ†æç²¾åº¦ï¼š{'AIå¢å¼ºæ¨¡å¼' if analyzer.use_ai_analysis else 'è§„åˆ™æ¨¡å¼'}\n"
    content += f"â€¢ é€‚ç”¨åœºæ™¯ï¼šè‡ªåŠ¨åŒ–å‰ªè¾‘ã€ç²¾å½©ç‰‡æ®µæå–ã€å†…å®¹åˆ†æ\n"

    # ä¿å­˜æŠ¥å‘Š
    safe_file_write('intelligent_analysis_report.txt', content)

    print(f"âœ… æ™ºèƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° intelligent_analysis_report.txt")
    print(f"ğŸ“„ æŠ¥å‘ŠåŒ…å«{len(episodes_plans)}é›†çš„è¯¦ç»†åˆ†æï¼Œæ€»æ—¶é•¿{total_duration/60:.1f}åˆ†é’Ÿ")

if __name__ == "__main__":
    analyze_all_episodes_intelligently()