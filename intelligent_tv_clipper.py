#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®Œæ•´æ™ºèƒ½ç”µè§†å‰§å‰ªè¾‘ç³»ç»Ÿ
è§£å†³æ‰€æœ‰æ ¸å¿ƒé—®é¢˜ï¼š
1. å®é™…å‰ªè¾‘ç”ŸæˆçŸ­è§†é¢‘å’Œæ—ç™½
2. å…¨æ–‡åˆ†æå‡å°‘APIè°ƒç”¨
3. ä¿è¯å‰§æƒ…è¿è´¯æ€§å’Œåè½¬å¤„ç†
4. ç”Ÿæˆä¸“ä¸šå‰§æƒ…åˆ†ææ—ç™½
5. ç¡®ä¿å¯¹è¯å®Œæ•´æ€§
"""

import os
import re
import json
import hashlib
import subprocess
from typing import List, Dict, Optional
from datetime import datetime
from unified_config import unified_config
from unified_ai_client import ai_client

class IntelligentTVClipper:
    def __init__(self):
        # ç›®å½•ç»“æ„
        self.srt_folder = "srt"
        self.video_folder = "videos"
        self.output_folder = "clips"
        self.cache_folder = "analysis_cache"
        self.narration_folder = "narrations"

        # åˆ›å»ºå¿…è¦ç›®å½•
        for folder in [self.srt_folder, self.video_folder, self.output_folder, 
                      self.cache_folder, self.narration_folder]:
            os.makedirs(folder, exist_ok=True)

        print("ğŸ¬ å®Œæ•´æ™ºèƒ½ç”µè§†å‰§å‰ªè¾‘ç³»ç»Ÿ")
        print("=" * 60)
        print(f"ğŸ“ å­—å¹•ç›®å½•: {self.srt_folder}/")
        print(f"ğŸ¥ è§†é¢‘ç›®å½•: {self.video_folder}/")
        print(f"âœ‚ï¸ è¾“å‡ºç›®å½•: {self.output_folder}/")
        print(f"ğŸ™ï¸ æ—ç™½ç›®å½•: {self.narration_folder}/")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_folder}/")

    def parse_subtitle_file(self, filepath: str) -> List[Dict]:
        """è§£æå­—å¹•æ–‡ä»¶ï¼Œæ™ºèƒ½ä¿®æ­£é”™è¯¯"""
        print(f"ğŸ“– è§£æå­—å¹•: {os.path.basename(filepath)}")

        # å°è¯•å¤šç§ç¼–ç è¯»å–
        content = None
        for encoding in ['utf-8', 'gbk', 'utf-16', 'gb2312']:
            try:
                with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                    break
            except:
                continue

        if not content:
            print("âŒ å­—å¹•æ–‡ä»¶è¯»å–å¤±è´¥")
            return []

        # æ™ºèƒ½é”™åˆ«å­—ä¿®æ­£
        corrections = {
            'é˜²è¡›': 'é˜²å«', 'æ­£ç•¶': 'æ­£å½“', 'è¨¼æ“š': 'è¯æ®', 'æª¢å¯Ÿå®˜': 'æ£€å¯Ÿå®˜',
            'ç™¼ç¾': 'å‘ç°', 'æ±ºå®š': 'å†³å®š', 'é¸æ“‡': 'é€‰æ‹©', 'é–‹å§‹': 'å¼€å§‹',
            'çµæŸ': 'ç»“æŸ', 'å•é¡Œ': 'é—®é¢˜', 'æ©Ÿæœƒ': 'æœºä¼š', 'è½è­‰æœƒ': 'å¬è¯ä¼š',
            'å“¡': 'å‘˜', 'æ•¸': 'æ•°', 'å‹™': 'åŠ¡', 'éšª': 'é™©', 'ç¨®': 'ç§'
        }

        for old, new in corrections.items():
            content = content.replace(old, new)

        # è§£æå­—å¹•æ¡ç›®
        subtitles = []
        blocks = re.split(r'\n\s*\n', content.strip())

        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) >= 3:
                try:
                    index = int(lines[0]) if lines[0].isdigit() else len(subtitles) + 1

                    time_pattern = r'(\d{2}:\d{2}:\d{2}[,\.]\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}[,\.]\d{3})'
                    time_match = re.search(time_pattern, lines[1])

                    if time_match:
                        start_time = time_match.group(1).replace('.', ',')
                        end_time = time_match.group(2).replace('.', ',')
                        text = '\n'.join(lines[2:]).strip()

                        if text:
                            subtitles.append({
                                'index': index,
                                'start': start_time,
                                'end': end_time,
                                'text': text,
                                'start_seconds': self._time_to_seconds(start_time),
                                'end_seconds': self._time_to_seconds(end_time)
                            })
                except:
                    continue

        print(f"âœ… è§£æå®Œæˆ: {len(subtitles)} æ¡å­—å¹•")
        return subtitles

    def get_analysis_cache_path(self, srt_file: str) -> str:
        """è·å–åˆ†æç¼“å­˜è·¯å¾„"""
        file_hash = self._get_file_hash(srt_file)
        base_name = os.path.splitext(os.path.basename(srt_file))[0]
        return os.path.join(self.cache_folder, f"{base_name}_{file_hash}_analysis.json")

    def _get_file_hash(self, filepath: str) -> str:
        """è·å–æ–‡ä»¶å†…å®¹å“ˆå¸Œå€¼"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            return hashlib.md5(content.encode()).hexdigest()[:16]
        except:
            return "unknown"

    def load_cached_analysis(self, srt_file: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜çš„åˆ†æç»“æœ"""
        cache_path = self.get_analysis_cache_path(srt_file)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    analysis = json.load(f)
                print(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜åˆ†æ: {os.path.basename(srt_file)}")
                return analysis
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None

    def save_analysis_cache(self, srt_file: str, analysis: Dict):
        """ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜"""
        cache_path = self.get_analysis_cache_path(srt_file)
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜åˆ†æç¼“å­˜: {os.path.basename(srt_file)}")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def ai_analyze_full_episode(self, subtitles: List[Dict], episode_name: str) -> Optional[Dict]:
        """AIåˆ†ææ•´é›†å†…å®¹ - å…¨æ–‡åˆ†æå‡å°‘APIè°ƒç”¨"""
        if not unified_config.is_enabled():
            print("âŒ AIæœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return None

        episode_num = self._extract_episode_number(episode_name)

        # æ„å»ºå®Œæ•´å‰§æƒ…æ–‡æœ¬ - å…¨æ–‡è¾“å…¥ä¿è¯è¿è´¯æ€§
        full_context = []
        for sub in subtitles:
            timestamp = f"[{sub['start']}]"
            full_context.append(f"{timestamp} {sub['text']}")

        complete_script = '\n'.join(full_context)

        # ä¸“ä¸šå‰§æƒ…åˆ†ææç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆå’Œå‰§æƒ…åˆ†æä¸“å®¶ã€‚è¯·å¯¹è¿™ä¸€é›†è¿›è¡Œæ·±åº¦åˆ†æï¼Œè¯†åˆ«å‡º3-4ä¸ªæœ€ç²¾å½©ä¸”è¿è´¯çš„ç‰‡æ®µç”¨äºçŸ­è§†é¢‘åˆ¶ä½œã€‚

ã€é›†æ•°ã€‘{episode_num}
ã€å®Œæ•´å‰§æƒ…å†…å®¹ã€‘
{complete_script}

è¯·è¿›è¡Œä¸“ä¸šåˆ†æå¹¶è¿”å›JSONæ ¼å¼ç»“æœï¼š

{{
    "episode_analysis": {{
        "episode_number": "{episode_num}",
        "main_storyline": "ä¸»è¦æ•…äº‹çº¿æè¿°",
        "key_characters": ["ä¸»è¦è§’è‰²1", "ä¸»è¦è§’è‰²2"],
        "plot_progression": ["æƒ…èŠ‚å‘å±•1", "æƒ…èŠ‚å‘å±•2"],
        "dramatic_arc": "æ•´ä½“æˆå‰§å¼§çº¿",
        "emotional_journey": "æƒ…æ„Ÿå†ç¨‹",
        "plot_twists": ["åè½¬ç‚¹1", "åè½¬ç‚¹2"],
        "continuity_points": ["ä¸å‰é›†è”ç³»", "ä¸ºåé›†é“ºå«"]
    }},
    "highlight_segments": [
        {{
            "segment_id": 1,
            "title": "ç‰‡æ®µæ ‡é¢˜",
            "start_time": "00:05:30,000",
            "end_time": "00:08:45,000",
            "plot_significance": "å‰§æƒ…é‡è¦æ€§è¯´æ˜",
            "dramatic_tension": 8.5,
            "emotional_impact": 9.0,
            "dialogue_quality": 8.0,
            "story_progression": "æ¨è¿›ä¸»çº¿å‰§æƒ…",
            "character_development": "è§’è‰²å‘å±•æè¿°",
            "visual_elements": ["è§†è§‰äº®ç‚¹1", "è§†è§‰äº®ç‚¹2"],
            "key_dialogues": [
                {{"speaker": "è§’è‰²å", "line": "å…³é”®å°è¯", "timestamp": "00:06:15,000"}},
                {{"speaker": "è§’è‰²å", "line": "é‡è¦å¯¹è¯", "timestamp": "00:07:20,000"}}
            ],
            "narrative_analysis": {{
                "setup": "æƒ…èŠ‚é“ºå«",
                "conflict": "æ ¸å¿ƒå†²çª",
                "climax": "é«˜æ½®éƒ¨åˆ†",
                "resolution": "è§£å†³æ–¹æ¡ˆ",
                "hook": "æ‚¬å¿µè®¾ç½®"
            }},
            "connection_to_series": {{
                "previous_reference": "ä¸å‰é¢å‰§æƒ…çš„è”ç³»",
                "future_setup": "ä¸ºåç»­æƒ…èŠ‚çš„é“ºå«",
                "series_importance": "åœ¨æ•´éƒ¨å‰§ä¸­çš„é‡è¦æ€§"
            }}
        }}
    ],
    "series_continuity": {{
        "previous_episode_connections": ["ä¸å‰é›†çš„è”ç³»1", "ä¸å‰é›†çš„è”ç³»2"],
        "next_episode_setup": ["ä¸ºä¸‹é›†çš„é“ºå«1", "ä¸ºä¸‹é›†çš„é“ºå«2"],
        "overarching_themes": ["æ€»ä½“ä¸»é¢˜1", "æ€»ä½“ä¸»é¢˜2"],
        "character_arcs": {{"è§’è‰²å": "è§’è‰²å‘å±•è½¨è¿¹"}}
    }},
    "narrative_coherence": {{
        "story_flow": "æ•…äº‹æµç•…æ€§è¯„ä¼°",
        "logical_consistency": "é€»è¾‘ä¸€è‡´æ€§",
        "emotional_consistency": "æƒ…æ„Ÿä¸€è‡´æ€§",
        "pacing_analysis": "èŠ‚å¥åˆ†æ"
    }}
}}

åˆ†æè¦æ±‚ï¼š
1. ç¡®ä¿ç‰‡æ®µæ—¶é—´åœ¨å­—å¹•èŒƒå›´å†…ä¸”æ ¼å¼ä¸ºHH:MM:SS,mmm
2. æ¯ä¸ªç‰‡æ®µ2-3åˆ†é’Ÿï¼ŒåŒ…å«å®Œæ•´çš„æˆå‰§ç»“æ„
3. ç‰‡æ®µä¹‹é—´è¦æœ‰é€»è¾‘è”ç³»ï¼Œèƒ½å®Œæ•´å™è¿°å‰§æƒ…
4. ç‰¹åˆ«å…³æ³¨åè½¬æƒ…èŠ‚ä¸å‰é¢å‰§æƒ…çš„è”ç³»
5. ç¡®ä¿å¯¹è¯å®Œæ•´æ€§ï¼Œä¸æˆªæ–­å¥å­
6. åˆ†æè¦æ·±åº¦ä¸“ä¸šï¼Œé€‚åˆåˆ¶ä½œæ—ç™½è§£è¯´"""

        try:
            print(f"ğŸ¤– AIåˆ†ææ•´é›†å†…å®¹...")
            response = ai_client.call_ai(prompt, "ä½ æ˜¯ä¸“ä¸šçš„ç”µè§†å‰§å‰ªè¾‘å¸ˆå’Œå‰§æƒ…åˆ†æä¸“å®¶ã€‚")

            if not response:
                print("âŒ AIåˆ†æå¤±è´¥")
                return None

            # è§£æAIå“åº”
            analysis = self._parse_ai_response(response)
            if not analysis:
                print("âŒ AIå“åº”è§£æå¤±è´¥")
                return None

            # éªŒè¯å’Œä¿®æ­£æ—¶é—´æ®µ
            validated_segments = []
            for segment in analysis.get('highlight_segments', []):
                if self._validate_and_fix_segment(segment, subtitles):
                    validated_segments.append(segment)

            if not validated_segments:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç‰‡æ®µ")
                return None

            analysis['highlight_segments'] = validated_segments
            return analysis

        except Exception as e:
            print(f"âŒ AIåˆ†æå‡ºé”™: {e}")
            return None

    def _parse_ai_response(self, response: str) -> Optional[Dict]:
        """è§£æAIå“åº”"""
        try:
            # æå–JSONå†…å®¹
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("